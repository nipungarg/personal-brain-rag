  Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to process sequences of tokens and capture long-range dependencies. Models like GPT-4, Claude, and Llama have billions of parameters and are trained on web text, books, and code. They can perform a wide range of tasks—summarization,,, translation, coding, reasoning—often with minimal or no task-specific training, a capability known as few-shot or zero-shot learning.

The transformer architecture, introduced in "Attention Is All You Need" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model can weigh relevant context regardless of distance. Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only models (e.g. GPT) are trained to predict the next token and are commonly used for general-purpose text generation.

Tokenization  converts raw text into discrete tokens that the model processes. Subword tokenizers (e.g. BPE, WordPiece, SentencePiece) split words into subword units, balancing vocabulary size and sequence length. The choice of tokenizer affects how much text fits in the context window and how well the model handles rare words or other languages. Token limits vary by model; exceeding them typically requires truncation, summarization, or chunking of long documents.

Prompt engineering is the practice of designing input text (prompts) to get desired outputs. Instructions, few-shot examples, and structured formats (e.g. "Answer in one sentence", "Step by step") can improve accuracy and consistency. System prompts set the model's role or behavior; user prompts carry the actual request. Techniques like chain-of-thought prompting ("Think step by step") can improve reasoning. Prompt injection is a security concern: malicious or accidental text in the prompt can override intended instructions.

Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long context enables processing of long documents, many-shot learning, and extended conversations. Drawbacks include higher cost, slower inference, and sometimes worse performance on information in the middle of very long contexts (the "lost in the middle" effect). Chunking long documents and using retrieval (e.g. RAG) is an alternative to stuffing everything into one context.

Fine-tuning adapts a pre-trained model to a specific task or domain. Full fine-tuning updates all parameters; parameter-efficient methods (e.g. LoRA, adapters) update only a small set of parameters, reducing cost and memory. Fine-tuning is used when prompt engineering is insufficient: custom tone, proprietary knowledge, or strict output formats. Instruction tuning trains the model to follow user instructions; reinforcement learning from human feedback (RLHF) aligns outputs with human preferences, reducing harmful or unhelpful responses.

RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. Given a query, a retrieval system (e.g. vector search over embeddings) fetches the most relevant passages from a corpus. Those passages are added to the prompt, and the LLM generates an answer grounded in them. RAG reduces hallucination and keeps knowledge up to date without retraining. Key components: document chunking, embedding model, vector store (e.g. Chroma, Pinecone), and the retrieval and prompt assembly logic. Chunk size and overlap, and whether to use single-hop or multi-hop retrieval, affect quality.

Hallucination is when an LLM produces plausible-sounding but incorrect or fabricated information. It can invent facts, citations, or events. Causes include limited or biased training data, overconfidence, and lack of grounding. Mitigations include RAG (grounding in retrieved text), asking the model to cite sources, calibration of confidence, and human review for high-stakes use. Evaluating hallucination requires benchmarks and human or model-based checks. As LLMs are deployed in search, support, and writing tools, managing hallucination remains an active area of research and product design.

  [REVIEW NEEDED]
***  Footnotes and citations to be added  ***
