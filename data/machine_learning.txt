  Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.   It focuses on developing algorithms that can access data, identify patterns, and make decisions with minimal human intervention. The term was coined in 1959 by Arthur Samuel, and the field has grown dramatically with the availability of large datasets and powerful computing.


Supervised learning is the most common paradigm. The  the algorithm learns from labeled examples: each training instance has an input and a known correct output. Classification (predicting a category, e.g.. spam or not spam) and regression (predicting a numeric value, e.g. house price) are the main tasks. Popular supervised algorithms include logistic regression, random forests, gradient boosting (XGBoost, LightGBM), and support vector machines. Deep learning models, such as convolutional neural networks (CNNs) for images and recurrent networks for sequences, are also supervised when trained with labeled data.

	Unsupervised learning works with unlabeled data. The goal is to discover hidden structure: clusters, dimensions, or anomalies. Clustering algorithms like K-means and hierarchical clustering group similar points together. Dimensionality reduction techniques such as PCA (Principal Component Analysis) and t-SNE compress data into fewer dimensions for visualization or preprocessing. Anomaly detection identifies rare or unusual examples that do not fit the general distribution. Unsupervised methods are useful when labels are expensive or unavailable.

Reinforcement learning (RL) models an agent that takes actions in an environment and receives rewards or penalties. The agent learns a policy that maximizes cumulative reward over time. RL has been successful in games (AlphaGo, game-playing bots), robotics, and resource allocation. Key ideas include value functions, policy gradient methods, and Q-learning. Exploration versus exploitation—trying new actions versus sticking to what works—is a central trade-off in RL.

Neural networks are function approximators composed of layers of interconnected nodes (neurons). Each connection has a weight that is learned from data. Activation functions introduce non-linearity (ReLU, sigmoid, softmax). Training typically uses backpropagation and gradient descent to minimize a loss function. Deep networks with many layers can learn hierarchical representations: early layers capture low-level features (edges, textures), and deeper layers capture high-level concepts (objects, scenes). This has driven progress in computer vision, speech, and natural language processing.

Evaluation and generalization are critical. Models are usually evaluated on a held-out test set to estimate performance on new data. Overfitting occurs when a model memorizes training data and performs poorly on unseen data; regularization (e.g. dropout, weight decay) and more data help. Cross-validation splits the data into folds to get more reliable estimates. Metrics depend on the task: accuracy, precision, recall, F1 for classification; MSE, MAE, R² for regression. Understanding bias, variance, and the bias–variance trade-off helps in model selection and tuning.

The field has broad applications. In computer vision: image classification, object detection, segmentation, and facial recognition. In natural language processing: sentiment analysis, machine translation, named entity recognition, and question answering. Recommendation systems suggest products, articles, or videos based on user behavior. In healthcare, ML aids in diagnosis, drug discovery, and personalized treatment. Autonomous vehicles rely on ML for perception, planning, and control. As data and compute grow, machine learning continues to expand into new domains and to combine with other disciplines such as causal inference and reinforcement learning.

--- [END OF DOCUMENT] ---
TODO: add references
Version: 1.0  Last updated: 2024-01-15
