  Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to process sequences of tokens and capture long-range dependencies. Models like GPT-4, Claude, and Llama have billions of parameters and are trained on web text, books, and code. They can perform a wide range of tasks—summarization,,, translation, coding, reasoning—often with minimal or no task-specific training, a capability known as few-shot or zero-shot learning.

The transformer architecture, introduced in "Attention Is All You Need" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model can weigh relevant context regardless of distance. In formal terms, self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension; this definition appears only in the concepts document and not in the practices guide. Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only models (e.g. GPT) are trained to predict the next token and are commonly used for general-purpose text generation.

Tokenization  converts raw text into discrete tokens that the model processes. Subword tokenizers (e.g. BPE, WordPiece, SentencePiece) split words into subword units, balancing vocabulary size and sequence length. The choice of tokenizer affects how much text fits in the context window and how well the model handles rare words or other languages. Token limits vary by model; exceeding them typically requires truncation, summarization, or chunking of long documents.

Prompt engineering is the practice of designing input text (prompts) to get desired outputs. Instructions, few-shot examples, and structured formats (e.g. "Answer in one sentence", "Step by step") can improve accuracy and consistency. System prompts set the model's role or behavior; user prompts carry the actual request. Techniques like chain-of-thought prompting ("Think step by step") can improve reasoning. Prompt injection is a security concern: malicious or accidental text in the prompt can override intended instructions.

Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long context enables processing of long documents, many-shot learning, and extended conversations. Drawbacks include higher cost, slower inference, and sometimes worse performance on information in the middle of very long contexts (the "lost in the middle" effect). Chunking long documents and using retrieval (e.g. RAG) is an alternative to stuffing everything into one context.

Fine-tuning adapts a pre-trained model to a specific task or domain. Full fine-tuning updates all parameters; parameter-efficient methods (e.g. LoRA, adapters) update only a small set of parameters, reducing cost and memory. Fine-tuning is used when prompt engineering is insufficient: custom tone, proprietary knowledge, or strict output formats. Instruction tuning trains the model to follow user instructions; reinforcement learning from human feedback (RLHF) aligns outputs with human preferences, reducing harmful or unhelpful responses.

RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(query) returns the top-k passages from the corpus; this formulation is given only in the concepts document. Given a query, a retrieval system (e.g. vector search over embeddings) fetches the most relevant passages from a corpus. Those passages are added to the prompt, and the LLM generates an answer grounded in them. RAG reduces hallucination and keeps knowledge up to date without retraining. Key components include document chunking, the embedding model, the vector store (e.g. Chroma, Pinecone), and the retrieval and prompt assembly logic. Chunk size and overlap, and whether you use single-hop or multi-hop retrieval, all affect the quality of answers you get.

• Document chunking: how you split long texts into retrievable units.
• Embedding model: turns text into vectors; use the same model at index and query time.
• Vector store: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant, etc.
• Top-k: how many chunks you retrieve per query; tune on validation.

Hallucination is when an LLM produces plausible-sounding but incorrect or fabricated information. It can invent facts, citations, or events. Causes include limited or biased training data, overconfidence, and lack of grounding. Mitigations include RAG (grounding in retrieved text), asking the model to cite sources, calibration of confidence, and human review for high-stakes use. Evaluating hallucination requires benchmarks and human or model-based checks. As LLMs are deployed in search, support, and writing tools, managing hallucination remains an active area of research and product design.

Embeddings are dense vector representations of text. Each piece of text—whether a sentence, a paragraph, or a chunk—is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embedding-3-small. Similar meanings produce similar vectors, so similarity search in vector space finds semantically related content. Embedding models can be general-purpose, such as those from OpenAI or Cohere, or domain-specific, trained on medical or legal text. Normalization (e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, Pinecone, Weaviate, and Qdrant. They support approximate nearest neighbour search so you can query with an embedding and get the top-k most similar stored vectors. The index type (flat versus IVF or HNSW) trades off accuracy for speed. Persistence and filtering by metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a great deal: too small and you lose context; too large and you dilute relevance. Overlap between chunks helps avoid splitting mid-sentence or mid-concept.

Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.

• Fixed character or token length: simple, but may cut sentences or topics in half; overlap (e.g. 50–100 tokens) reduces boundary effects.
• Sentence or paragraph boundaries: keeps semantic units intact; use a sentence splitter (spaCy, LangChain) then group to target size.
• Semantic chunking: split where embedding similarity drops so each chunk is a coherent theme; more compute, often better retrieval.
• Section-based: split on Markdown or HTML headings; good for structured docs.

Small chunks (100–200 tokens) give more precise retrieval but you may need higher k or several chunks to answer. Large chunks (500–1000 tokens) give more context per chunk but more irrelevant text. For RAG tutorials try chunk sizes 100–500 and overlap 0–50 to see the effect.

The value k in "top-k retrieval" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger k gives the LLM more context and can improve recall because you are more likely to include the right passage, but it adds noise and cost. Smaller k keeps the prompt focused but risks missing the answer if it lies in the second or third best chunk.

• Short factual questions: k = 2 or 3 is often enough.
• Complex or multi-part questions: k = 5–10 or multi-hop retrieval may help.
• Always tune k on a validation set and measure accuracy or faithfulness.

RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents and using retrieval such as RAG is an alternative to stuffing everything into one context, so RAG is one way to handle long context. We also said that mitigations for hallucination include RAG, by grounding answers in retrieved text. In the main RAG section we listed key components: document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k values shows how retrieval and answers change. Scattered information like this tests whether your chunks and your choice of k capture all relevant mentions across the document.

After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever casts a wide net and the re-ranker picks the best few. Hybrid search combines dense (embedding) retrieval with sparse (keyword or BM25) retrieval. Queries that need exact matches, for example names or IDs, benefit from sparse retrieval; semantic questions benefit from dense. Fusion methods such as reciprocal rank fusion or a weighted combination merge the two result lists. This is useful when your corpus has both narrative and structured content.

Metrics for evaluating RAG systems include retrieval recall (is the gold passage in the top-k?), retrieval precision (how many of the top-k are relevant?), faithfulness (does the generated answer stay grounded in the retrieved text?), answer relevance (does the answer address the question?), and citation accuracy (do cited chunks support the claim?). You can use a small labeled set of query–document–answer triples, run retrieval and generation, and compute these metrics. Varying chunk size and k and comparing the results is instructive. Data cleaning, such as normalizing whitespace and removing boilerplate, can improve embedding quality and retrieval; it is worth testing with and without cleaning to see the impact.

(1) Retrieval recall: is the gold passage in the top-k?
(2) Retrieval precision: how many of the top-k are relevant?
(3) Faithfulness: does the answer stay grounded in the retrieved text?
(4) Answer relevance: does the answer address the question?
(5) Citation accuracy: do cited chunks support the claim?

This paragraph is intentionally off-topic. When building a RAG pipeline, you might ingest documents that contain boilerplate, copyright notices, or unrelated sections. Good preprocessing and chunking should reduce the chance that such content is retrieved for a semantic query. If your retriever returns this paragraph for a question about what RAG is, your chunk size or k might need tuning, or your cleaning step might need to drop such blocks. You can consider filtering by section headers or by simple heuristics such as minimum length or excluding lines that are only numbers or dates. CSV and JSON are common formats for exporting document chunks; version 1.2.3, last updated 2024. TODO: add more examples.

In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the prompt, and generating; it reduces hallucination and requires decisions about chunking, embeddings, vector store, and the choice of k. Chunk size and overlap trade off context versus precision; a practical range is 100 to 500 tokens with overlap 0 to 50. For top-k, higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chosen by scale and features. Re-ranking and hybrid search are optional steps that can improve precision or handle mixed query types. Evaluation should consider retrieval recall and precision, faithfulness, relevance, and citations, and it is useful to test with and without data cleaning.

• LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.
• RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, choice of k.
• Chunk size and overlap: try 100–500 tokens, overlap 0–50.
• Top-k: tune on validation; higher k = more context, more noise.
• Embeddings and vector stores: same model for index and query.
• Re-ranking and hybrid search: optional; improve precision or handle mixed queries.
• Evaluation: recall, precision, faithfulness, relevance, citations; test with and without cleaning.

Exclusive to this file (LLM Concepts only): the "information bottleneck" in RAG is the retriever—the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bounds answer quality. Multi-hop retrieval is defined here as issuing follow-up queries (using the initial retrieved chunks or the model's intermediate answer) and then combining results from multiple retrieval rounds; this concept is not defined in the practices document. Contrastive learning is the typical training objective for embedding models: relevant (query, passage) pairs are pulled together in vector space and irrelevant pairs are pushed apart. These definitions and the formal p(answer | query, retrieve(query)) formulation appear only in the concepts document.

  [REVIEW NEEDED]
***  Footnotes and citations to be added  ***
--- [END OF DOCUMENT] ---
TODO: add references
Version: 1.0  Last updated: 2024-01-15
