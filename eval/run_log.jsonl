{"timestamp": "2026-02-16T10:34:02.529561", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.6960015296936035, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8462702035903931, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8555042743682861, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8603806495666504, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.4159}}
{"timestamp": "2026-02-16T10:34:02.801082", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6260308027267456, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6634612083435059, "chunk_id": "llm_concepts_14", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.7075117826461792, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.74504554271698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": false, "time_sec": 0.2711}}
{"timestamp": "2026-02-16T10:34:03.061269", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.734548807144165, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8653066158294678, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9895203113555908, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1145915985107422, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2599}}
{"timestamp": "2026-02-16T10:34:03.836399", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8328152894973755, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.868087887763977, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8998264074325562, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0441789627075195, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": false, "time_sec": 0.7745}}
{"timestamp": "2026-02-16T10:34:04.085259", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.530610203742981, "chunk_id": "llm_concepts_16", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7539793848991394, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8223064541816711, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8402668237686157, "chunk_id": "llm_practices_11", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2484}}
{"timestamp": "2026-02-16T10:34:04.325969", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.9059703946113586, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9893731474876404, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0064488649368286, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0320048332214355, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2402}}
{"timestamp": "2026-02-16T10:34:04.559662", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8350740671157837, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8576250076293945, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8643407821655273, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8861725330352783, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2332}}
{"timestamp": "2026-02-16T10:34:04.778094", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.8032857179641724, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.114275574684143, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.118534803390503, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1281572580337524, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2179}}
{"timestamp": "2026-02-16T10:34:05.021513", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9853620529174805, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9971528053283691, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0357369184494019, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1020700931549072, "chunk_id": "llm_concepts_5", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2429}}
{"timestamp": "2026-02-16T10:34:05.251364", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.5392513871192932, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1736438274383545, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2430522441864014, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2915929555892944, "chunk_id": "llm_concepts_24", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": false, "time_sec": 0.2295}}
{"timestamp": "2026-02-16T10:34:05.472659", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.6440763473510742, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6456518769264221, "chunk_id": "llm_practices_9", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8316242694854736, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.866807222366333, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": false, "time_sec": 0.2208}}
{"timestamp": "2026-02-16T10:34:05.769804", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2606755495071411, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.284359097480774, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.358940839767456, "chunk_id": "llm_practices_16", "text_snippet": "depend on exact phrases or entity names, sparse search helps; for conceptual or semantic questions, dense search is better. Fusing the two result lists (e.g. reciprocal rank fusion) gives you a single...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.362554669380188, "chunk_id": "llm_practices_8", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": false, "time_sec": 0.2966}}
{"timestamp": "2026-02-16T10:34:06.005917", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.0910089015960693, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2192013263702393, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2618955373764038, "chunk_id": "llm_concepts_25", "text_snippet": "higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chos...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": false, "time_sec": 0.2358}}
{"timestamp": "2026-02-16T10:34:06.257386", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2702752351760864, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2737611532211304, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.300235629081726, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": false, "time_sec": 0.251}}
{"timestamp": "2026-02-16T10:34:06.499089", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.9360617399215698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0198891162872314, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0521656274795532, "chunk_id": "llm_practices_21", "text_snippet": "all documents in the prompt; chunking and retrieval select the most relevant subset. Choosing the right chunk size and top-k so that the model receives enough context without overwhelming it is a cent...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.0763764381408691, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2412}}
{"timestamp": "2026-02-16T10:34:06.730741", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "results": [{"rank": 1, "score": 1.8505175113677979, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.8667559623718262, "chunk_id": "llm_concepts_4", "text_snippet": "Prompt engineering is the practice of designing input text (prompts) to get desired outputs. Instructions, few-shot examples, and structured formats (e.g. \"Answer in one sentence\", \"Step by step\") can...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.8722002506256104, "chunk_id": "machine_learning_3", "text_snippet": "Reinforcement learning (RL) models an agent that takes actions in an environment and receives rewards or penalties. The agent learns a policy that maximizes cumulative reward over time. RL has been su...", "source": "machine_learning.txt"}, {"rank": 4, "score": 1.8819547891616821, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}], "retrieved_sources": ["machine_learning.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2312}}
{"timestamp": "2026-02-16T10:34:06.978719", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "results": [{"rank": 1, "score": 1.7541056871414185, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.7692875862121582, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.8011770248413086, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.8300684690475464, "chunk_id": "machine_learning_4", "text_snippet": "Neural networks are function approximators composed of layers of interconnected nodes (neurons). Each connection has a weight that is learned from data. Activation functions introduce non-linearity (R...", "source": "machine_learning.txt"}], "retrieved_sources": ["llm_concepts.txt", "machine_learning.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2473}}
{"timestamp": "2026-02-16T10:34:07.256078", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "results": [{"rank": 1, "score": 1.7459312677383423, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.7826188802719116, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.8441693782806396, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.8589329719543457, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2768}}
{"timestamp": "2026-02-16T10:36:54.592877", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.6960015296936035, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8462702035903931, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8555042743682861, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8603806495666504, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.396}}
{"timestamp": "2026-02-16T10:36:55.231648", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6260308027267456, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6634612083435059, "chunk_id": "llm_concepts_14", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.7075117826461792, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.74504554271698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.6383}}
{"timestamp": "2026-02-16T10:36:55.519392", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.7347018718719482, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.865385890007019, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9895541667938232, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1145764589309692, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2872}}
{"timestamp": "2026-02-16T10:36:56.101632", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8328152894973755, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.868087887763977, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8998264074325562, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0441789627075195, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.5815}}
{"timestamp": "2026-02-16T10:36:56.342453", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.530610203742981, "chunk_id": "llm_concepts_16", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7539793848991394, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8223064541816711, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8402668237686157, "chunk_id": "llm_practices_11", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2403}}
{"timestamp": "2026-02-16T10:36:56.587545", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.9059703946113586, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9893731474876404, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0064488649368286, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0320048332214355, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2446}}
{"timestamp": "2026-02-16T10:36:56.807623", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8350740671157837, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8576250076293945, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8643407821655273, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8861725330352783, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2195}}
{"timestamp": "2026-02-16T10:36:57.560556", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.8032857179641724, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.114275574684143, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.118534803390503, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1281572580337524, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.7526}}
{"timestamp": "2026-02-16T10:36:57.796292", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9853620529174805, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9971528053283691, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0357369184494019, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1020700931549072, "chunk_id": "llm_concepts_5", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2351}}
{"timestamp": "2026-02-16T10:36:58.021460", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.5392513871192932, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1736438274383545, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2430522441864014, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2915929555892944, "chunk_id": "llm_concepts_24", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": false, "time_sec": 0.2247}}
{"timestamp": "2026-02-16T10:36:58.242241", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.6440763473510742, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6456518769264221, "chunk_id": "llm_practices_9", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8316242694854736, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.866807222366333, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2204}}
{"timestamp": "2026-02-16T10:36:58.491212", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2606570720672607, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842625379562378, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.3589427471160889, "chunk_id": "llm_practices_16", "text_snippet": "depend on exact phrases or entity names, sparse search helps; for conceptual or semantic questions, dense search is better. Fusing the two result lists (e.g. reciprocal rank fusion) gives you a single...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3625268936157227, "chunk_id": "llm_practices_8", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": false, "time_sec": 0.2483}}
{"timestamp": "2026-02-16T10:36:58.712662", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.0910089015960693, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2192013263702393, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2618955373764038, "chunk_id": "llm_concepts_25", "text_snippet": "higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chos...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.221}}
{"timestamp": "2026-02-16T10:36:58.930358", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2702752351760864, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2737611532211304, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.300235629081726, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2168}}
{"timestamp": "2026-02-16T10:36:59.147079", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.9360617399215698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0198891162872314, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0521656274795532, "chunk_id": "llm_practices_21", "text_snippet": "all documents in the prompt; chunking and retrieval select the most relevant subset. Choosing the right chunk size and top-k so that the model receives enough context without overwhelming it is a cent...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.0763764381408691, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2163}}
{"timestamp": "2026-02-16T10:36:59.397663", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "results": [{"rank": 1, "score": 1.8505175113677979, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.8667559623718262, "chunk_id": "llm_concepts_4", "text_snippet": "Prompt engineering is the practice of designing input text (prompts) to get desired outputs. Instructions, few-shot examples, and structured formats (e.g. \"Answer in one sentence\", \"Step by step\") can...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.8722002506256104, "chunk_id": "machine_learning_3", "text_snippet": "Reinforcement learning (RL) models an agent that takes actions in an environment and receives rewards or penalties. The agent learns a policy that maximizes cumulative reward over time. RL has been su...", "source": "machine_learning.txt"}, {"rank": 4, "score": 1.8819547891616821, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}], "retrieved_sources": ["machine_learning.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2502}}
{"timestamp": "2026-02-16T10:37:00.292121", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "results": [{"rank": 1, "score": 1.7541056871414185, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.7692875862121582, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.8011770248413086, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.8300684690475464, "chunk_id": "machine_learning_4", "text_snippet": "Neural networks are function approximators composed of layers of interconnected nodes (neurons). Each connection has a weight that is learned from data. Activation functions introduce non-linearity (R...", "source": "machine_learning.txt"}], "retrieved_sources": ["machine_learning.txt", "llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.8938}}
{"timestamp": "2026-02-16T10:37:00.506479", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "results": [{"rank": 1, "score": 1.7459312677383423, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.7826188802719116, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.8441693782806396, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.8589329719543457, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2139}}
{"timestamp": "2026-02-16T11:09:21.768458", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.6960015296936035, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8462702035903931, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8555042743682861, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8603806495666504, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.5414}}
{"timestamp": "2026-02-16T11:09:22.036192", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6260308027267456, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6634612083435059, "chunk_id": "llm_concepts_14", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.7075117826461792, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.74504554271698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2673}}
{"timestamp": "2026-02-16T11:09:22.316108", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.734548807144165, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8653066158294678, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9895203113555908, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1145915985107422, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2795}}
{"timestamp": "2026-02-16T11:09:22.579852", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8328152894973755, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.868087887763977, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8998264074325562, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0441789627075195, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2625}}
{"timestamp": "2026-02-16T11:09:22.837769", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.530610203742981, "chunk_id": "llm_concepts_16", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7539793848991394, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8223064541816711, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8402668237686157, "chunk_id": "llm_practices_11", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2573}}
{"timestamp": "2026-02-16T11:09:23.064596", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.9059703946113586, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9893731474876404, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0064488649368286, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0320048332214355, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2263}}
{"timestamp": "2026-02-16T11:09:23.279451", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8350740671157837, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8576250076293945, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8643407821655273, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8861725330352783, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2142}}
{"timestamp": "2026-02-16T11:09:23.542745", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.8032857179641724, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.114275574684143, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.118534803390503, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1281572580337524, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2628}}
{"timestamp": "2026-02-16T11:09:23.765531", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9853620529174805, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9971528053283691, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0357369184494019, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1020700931549072, "chunk_id": "llm_concepts_5", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2223}}
{"timestamp": "2026-02-16T11:09:24.011256", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.5392513871192932, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1736438274383545, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2430522441864014, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2915929555892944, "chunk_id": "llm_concepts_24", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.2453}}
{"timestamp": "2026-02-16T11:09:24.235142", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.6440763473510742, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6456518769264221, "chunk_id": "llm_practices_9", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8316242694854736, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.866807222366333, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2234}}
{"timestamp": "2026-02-16T11:09:24.462469", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2606570720672607, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842625379562378, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.3589427471160889, "chunk_id": "llm_practices_16", "text_snippet": "depend on exact phrases or entity names, sparse search helps; for conceptual or semantic questions, dense search is better. Fusing the two result lists (e.g. reciprocal rank fusion) gives you a single...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3625268936157227, "chunk_id": "llm_practices_8", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2268}}
{"timestamp": "2026-02-16T11:09:24.689043", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.0910089015960693, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2192013263702393, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2618955373764038, "chunk_id": "llm_concepts_25", "text_snippet": "higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chos...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2261}}
{"timestamp": "2026-02-16T11:09:24.973545", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2702752351760864, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2737611532211304, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.300235629081726, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.284}}
{"timestamp": "2026-02-16T11:09:25.259349", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.9360617399215698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0198891162872314, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0521656274795532, "chunk_id": "llm_practices_21", "text_snippet": "all documents in the prompt; chunking and retrieval select the most relevant subset. Choosing the right chunk size and top-k so that the model receives enough context without overwhelming it is a cent...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.0763764381408691, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2854}}
{"timestamp": "2026-02-16T11:09:26.896663", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.228, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:09:28.372724", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.243, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:09:29.784731", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2476, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:09:53.875384", "event": "retrieval_eval", "data": {"backend": "query", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.5661568949595326, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.5624106973485679, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.5227201645096521, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.5035012070109387, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2436}}
{"timestamp": "2026-02-16T11:09:54.197731", "event": "retrieval_eval", "data": {"backend": "query", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.5751418449463721, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.5554266090573726, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.5238167881985817, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.5157147171088621, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3219}}
{"timestamp": "2026-02-16T11:09:54.444649", "event": "retrieval_eval", "data": {"backend": "query", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.5133896164382529, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.43252791984051336, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.4141391686416844, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.41355970349919297, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2464}}
{"timestamp": "2026-02-16T11:09:54.711740", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.4502043627340987, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.44892427482130925, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.4349208544308343, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.41998655839217996, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2667}}
{"timestamp": "2026-02-16T11:09:54.949527", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.5712645963804172, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.5600474181403573, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.5450027879825441, "chunk_id": "llm_practices_2", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.539301661237801, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2374}}
{"timestamp": "2026-02-16T11:09:55.167259", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.48722053845209945, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.46930529337947846, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.4590911013579471, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.45610476623296986, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2173}}
{"timestamp": "2026-02-16T11:09:55.489120", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.5546132928035314, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.528676923179718, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.5134882655208862, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.5069751086266173, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3215}}
{"timestamp": "2026-02-16T11:09:55.718855", "event": "retrieval_eval", "data": {"backend": "query", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.5009329833600669, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.3994121592871458, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.38591773434852844, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.37538033288558265, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2293}}
{"timestamp": "2026-02-16T11:09:55.948886", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.4970318463744991, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.4591750062469108, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.43803194556122227, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.4281196440765801, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2296}}
{"timestamp": "2026-02-16T11:09:56.206873", "event": "retrieval_eval", "data": {"backend": "query", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.49849346770877123, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.3331226443059751, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.32667113679414117, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.3227356610224027, "chunk_id": "machine_learning_0", "text_snippet": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can acce...", "source": "machine_learning.txt"}], "retrieved_sources": ["machine_learning.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2575}}
{"timestamp": "2026-02-16T11:09:56.479850", "event": "retrieval_eval", "data": {"backend": "query", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.5566293170837329, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.5523606528288622, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.5306682024843566, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.5298268250021285, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2725}}
{"timestamp": "2026-02-16T11:09:56.696761", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 0.3316961151214666, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.3089683915017665, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.2940204991273256, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.29229830325176165, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2164}}
{"timestamp": "2026-02-16T11:09:56.950077", "event": "retrieval_eval", "data": {"backend": "query", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 0.3932182494787031, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.3122904497407709, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.2491971654022775, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.24469499185901203, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2529}}
{"timestamp": "2026-02-16T11:09:57.205603", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.48373474861542026, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.3207852703748373, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.303692898155118, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.28970261840463896, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2551}}
{"timestamp": "2026-02-16T11:09:57.438588", "event": "retrieval_eval", "data": {"backend": "query", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.46973209260908216, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.4314125499012694, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.4059718788133446, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.3817046112369801, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2326}}
{"timestamp": "2026-02-16T11:10:00.189824", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is the capital of France?", "results": [{"rank": 1, "score": 0.06090740264569302, "chunk_id": "llm_practices_2", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.046817881215352464, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.0419063803892901, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.039642331676959436, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2162, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:10:02.555986", "event": "retrieval_eval", "data": {"backend": "query", "question": "How do you bake a chocolate cake?", "results": [{"rank": 1, "score": 0.0888244576760161, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.06682298077281884, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.06301659981824886, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.05918620233283186, "chunk_id": "llm_practices_2", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2409, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:10:05.732762", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is the population of Tokyo?", "results": [{"rank": 1, "score": 0.08153758058927367, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.04280149443256491, "chunk_id": "llm_practices_2", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.04184803142925848, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.04067126874298195, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.4285, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:10:14.068748", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.6960015296936035, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.845935583114624, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8555042743682861, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8604533672332764, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.411}}
{"timestamp": "2026-02-16T11:10:14.333285", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6260308027267456, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6579909324645996, "chunk_id": "llm_concepts_14", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.7077221274375916, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.74504554271698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2641}}
{"timestamp": "2026-02-16T11:10:14.603797", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.734596848487854, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8653779625892639, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9895203113555908, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1144559383392334, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2699}}
{"timestamp": "2026-02-16T11:10:14.822536", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8328428268432617, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.868087887763977, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8998264074325562, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0441789627075195, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2181}}
{"timestamp": "2026-02-16T11:10:15.050268", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.5305343866348267, "chunk_id": "llm_concepts_16", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7539218068122864, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8221240043640137, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8402941226959229, "chunk_id": "llm_practices_11", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2271}}
{"timestamp": "2026-02-16T11:10:15.271752", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.9059703350067139, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9893732070922852, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0060703754425049, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0320217609405518, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2209}}
{"timestamp": "2026-02-16T11:10:15.485458", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8350740671157837, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8572756052017212, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8645462989807129, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8861725330352783, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.213}}
{"timestamp": "2026-02-16T11:10:15.698264", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.8032724261283875, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.114275574684143, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1185452938079834, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1281483173370361, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2122}}
{"timestamp": "2026-02-16T11:10:15.947219", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9852291345596313, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9971412420272827, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0357550382614136, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1020684242248535, "chunk_id": "llm_concepts_5", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2484}}
{"timestamp": "2026-02-16T11:10:16.172922", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.5392513871192932, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1737817525863647, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2430522441864014, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2915928363800049, "chunk_id": "llm_concepts_24", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.225}}
{"timestamp": "2026-02-16T11:10:16.405465", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.644150972366333, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6456081867218018, "chunk_id": "llm_practices_9", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8316242694854736, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.8661966919898987, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2319}}
{"timestamp": "2026-02-16T11:10:16.676051", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2603691816329956, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842626571655273, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.358922004699707, "chunk_id": "llm_practices_16", "text_snippet": "depend on exact phrases or entity names, sparse search helps; for conceptual or semantic questions, dense search is better. Fusing the two result lists (e.g. reciprocal rank fusion) gives you a single...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.362496018409729, "chunk_id": "llm_practices_8", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2699}}
{"timestamp": "2026-02-16T11:10:16.926443", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.0910089015960693, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2191529273986816, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2622498273849487, "chunk_id": "llm_concepts_25", "text_snippet": "higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chos...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2497}}
{"timestamp": "2026-02-16T11:10:17.141505", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2702752351760864, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.273678183555603, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3002434968948364, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2146}}
{"timestamp": "2026-02-16T11:10:17.388538", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.9360617399215698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.019889235496521, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0520596504211426, "chunk_id": "llm_practices_21", "text_snippet": "all documents in the prompt; chunking and retrieval select the most relevant subset. Choosing the right chunk size and top-k so that the model receives enough context without overwhelming it is a cent...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.0763764381408691, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2466}}
{"timestamp": "2026-02-16T11:10:18.556064", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the capital of France?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2152, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:10:19.756118", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "How do you bake a chocolate cake?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2536, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:10:22.362195", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the population of Tokyo?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.3083, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-17T02:08:50.697945", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 3.5068, "answer_snippet": "RAG solves the problem of hallucination by grounding answers in retrieved text, allowing the model to cite sources and stay up to date without retraining."}}
{"timestamp": "2026-02-17T02:08:53.875477", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 3.1768, "answer_snippet": "Chunking documents is done to split long texts into retrievable units that fit the retrieval step and the context window. It helps maintain relevant context while ensuring that each chunk is manageabl..."}}
{"timestamp": "2026-02-17T02:08:56.221172", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.345, "answer_snippet": "Embeddings are dense vector representations of text, where each piece of text\u2014such as a sentence or paragraph\u2014is mapped to a fixed-size vector. Similar meanings produce similar vectors, enabling simil..."}}
{"timestamp": "2026-02-17T02:08:57.765436", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.5436, "answer_snippet": "Chunk overlap is used to ensure that sentences are not cut in the middle, preserving surrounding context and improving the precision of retrieval."}}
{"timestamp": "2026-02-17T02:09:01.174883", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 3.409, "answer_snippet": "Top-k in retrieval refers to the value k, which determines how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, the ..."}}
{"timestamp": "2026-02-17T02:09:02.987801", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.8123, "answer_snippet": "RAG (Retrieval-Augmented Generation) can be described in formal terms as modeling p(answer | query, retrieve(query)), where retrieve(query) returns the top-k passages from the corpus."}}
{"timestamp": "2026-02-17T02:09:04.223374", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 1.2351, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T02:09:06.193559", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.9697, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries using the initial retrieved chunks or the model's intermediate answer and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T02:09:08.365866", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 2.1719, "answer_snippet": "Contrastive learning is the typical training objective for embedding models where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T02:09:10.214717", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.8485, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T02:09:11.872300", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.6572, "answer_snippet": "The RAG practices guide recommends a default chunk size of 256 tokens and an overlap of 32 tokens."}}
{"timestamp": "2026-02-17T02:09:13.597161", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.7242, "answer_snippet": "Query rewriting is the process of rephrasing the user question into a more retrieval-friendly form before embedding. It can improve recall, making it easier to retrieve relevant information."}}
{"timestamp": "2026-02-17T02:09:15.583847", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.9861, "answer_snippet": "Use a flat index for latency-sensitive applications with corpora under roughly 100k chunks. Switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T02:09:17.412347", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.828, "answer_snippet": "The incremental indexing rule defines that when you add or update documents, you should only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk ..."}}
{"timestamp": "2026-02-17T02:09:18.997325", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.5844, "answer_snippet": "Storing chunk_id in vector store metadata is important so that the model can reference the correct document and the user can verify the source of the information."}}
{"timestamp": "2026-02-17T02:09:19.987136", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 0.9892, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T02:09:20.935282", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 0.9475, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T02:09:21.893535", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 0.9576, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T02:09:31.740740", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.6960015296936035, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.845935583114624, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8555042743682861, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8604533672332764, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.453}}
{"timestamp": "2026-02-17T02:09:31.954383", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6260308027267456, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6579909324645996, "chunk_id": "llm_concepts_14", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.7077221274375916, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.74504554271698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2133}}
{"timestamp": "2026-02-17T02:09:32.173350", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.734596848487854, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8653779625892639, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9895203113555908, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1144559383392334, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2184}}
{"timestamp": "2026-02-17T02:09:32.409220", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8328428268432617, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.868087887763977, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8998264074325562, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0441789627075195, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2353}}
{"timestamp": "2026-02-17T02:09:32.629022", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.5305343866348267, "chunk_id": "llm_concepts_16", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7539218068122864, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8221240043640137, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8402941226959229, "chunk_id": "llm_practices_11", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2193}}
{"timestamp": "2026-02-17T02:09:32.875587", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.9059703350067139, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9893732070922852, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0060703754425049, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0320217609405518, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2461}}
{"timestamp": "2026-02-17T02:09:33.152180", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8350740671157837, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8572756052017212, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8645462989807129, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8861725330352783, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.276}}
{"timestamp": "2026-02-17T02:09:33.412991", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.8032724261283875, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.114275574684143, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1185452938079834, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1281483173370361, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2602}}
{"timestamp": "2026-02-17T02:09:33.631560", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9852291345596313, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9971412420272827, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0357550382614136, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1020684242248535, "chunk_id": "llm_concepts_5", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.218}}
{"timestamp": "2026-02-17T02:09:33.845542", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.5392513871192932, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1737817525863647, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2430522441864014, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2915928363800049, "chunk_id": "llm_concepts_24", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.2133}}
{"timestamp": "2026-02-17T02:09:34.067651", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.644150972366333, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6456081867218018, "chunk_id": "llm_practices_9", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8316242694854736, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.8661966919898987, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2214}}
{"timestamp": "2026-02-17T02:09:34.334803", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.260318398475647, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842984199523926, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.3588334321975708, "chunk_id": "llm_practices_16", "text_snippet": "depend on exact phrases or entity names, sparse search helps; for conceptual or semantic questions, dense search is better. Fusing the two result lists (e.g. reciprocal rank fusion) gives you a single...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3624961376190186, "chunk_id": "llm_practices_8", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2665}}
{"timestamp": "2026-02-17T02:09:34.734148", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0002903938293457, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.0910131931304932, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2192342281341553, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.262299656867981, "chunk_id": "llm_concepts_25", "text_snippet": "higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chos...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3987}}
{"timestamp": "2026-02-17T02:09:35.040711", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2702752351760864, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.273678183555603, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3002434968948364, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.306}}
{"timestamp": "2026-02-17T02:09:35.300568", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.9360617399215698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.019889235496521, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0520596504211426, "chunk_id": "llm_practices_21", "text_snippet": "all documents in the prompt; chunking and retrieval select the most relevant subset. Choosing the right chunk size and top-k so that the model receives enough context without overwhelming it is a cent...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.0763764381408691, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2593}}
{"timestamp": "2026-02-17T02:09:35.556289", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the capital of France?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2551}}
{"timestamp": "2026-02-17T02:09:35.862501", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "How do you bake a chocolate cake?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.3057}}
{"timestamp": "2026-02-17T02:09:36.101247", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the population of Tokyo?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2381}}
