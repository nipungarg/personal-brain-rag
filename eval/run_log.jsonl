{"timestamp": "2026-02-16T10:34:02.529561", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.6960015296936035, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8462702035903931, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8555042743682861, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8603806495666504, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.4159}}
{"timestamp": "2026-02-16T10:34:02.801082", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6260308027267456, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6634612083435059, "chunk_id": "llm_concepts_14", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.7075117826461792, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.74504554271698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": false, "time_sec": 0.2711}}
{"timestamp": "2026-02-16T10:34:03.061269", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.734548807144165, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8653066158294678, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9895203113555908, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1145915985107422, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2599}}
{"timestamp": "2026-02-16T10:34:03.836399", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8328152894973755, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.868087887763977, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8998264074325562, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0441789627075195, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": false, "time_sec": 0.7745}}
{"timestamp": "2026-02-16T10:34:04.085259", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.530610203742981, "chunk_id": "llm_concepts_16", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7539793848991394, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8223064541816711, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8402668237686157, "chunk_id": "llm_practices_11", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2484}}
{"timestamp": "2026-02-16T10:34:04.325969", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.9059703946113586, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9893731474876404, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0064488649368286, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0320048332214355, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2402}}
{"timestamp": "2026-02-16T10:34:04.559662", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8350740671157837, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8576250076293945, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8643407821655273, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8861725330352783, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2332}}
{"timestamp": "2026-02-16T10:34:04.778094", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.8032857179641724, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.114275574684143, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.118534803390503, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1281572580337524, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2179}}
{"timestamp": "2026-02-16T10:34:05.021513", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9853620529174805, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9971528053283691, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0357369184494019, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1020700931549072, "chunk_id": "llm_concepts_5", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2429}}
{"timestamp": "2026-02-16T10:34:05.251364", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.5392513871192932, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1736438274383545, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2430522441864014, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2915929555892944, "chunk_id": "llm_concepts_24", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": false, "time_sec": 0.2295}}
{"timestamp": "2026-02-16T10:34:05.472659", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.6440763473510742, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6456518769264221, "chunk_id": "llm_practices_9", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8316242694854736, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.866807222366333, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": false, "time_sec": 0.2208}}
{"timestamp": "2026-02-16T10:34:05.769804", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2606755495071411, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.284359097480774, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.358940839767456, "chunk_id": "llm_practices_16", "text_snippet": "depend on exact phrases or entity names, sparse search helps; for conceptual or semantic questions, dense search is better. Fusing the two result lists (e.g. reciprocal rank fusion) gives you a single...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.362554669380188, "chunk_id": "llm_practices_8", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": false, "time_sec": 0.2966}}
{"timestamp": "2026-02-16T10:34:06.005917", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.0910089015960693, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2192013263702393, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2618955373764038, "chunk_id": "llm_concepts_25", "text_snippet": "higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chos...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": false, "time_sec": 0.2358}}
{"timestamp": "2026-02-16T10:34:06.257386", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2702752351760864, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2737611532211304, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.300235629081726, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": false, "time_sec": 0.251}}
{"timestamp": "2026-02-16T10:34:06.499089", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.9360617399215698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0198891162872314, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0521656274795532, "chunk_id": "llm_practices_21", "text_snippet": "all documents in the prompt; chunking and retrieval select the most relevant subset. Choosing the right chunk size and top-k so that the model receives enough context without overwhelming it is a cent...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.0763764381408691, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2412}}
{"timestamp": "2026-02-16T10:34:06.730741", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "results": [{"rank": 1, "score": 1.8505175113677979, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.8667559623718262, "chunk_id": "llm_concepts_4", "text_snippet": "Prompt engineering is the practice of designing input text (prompts) to get desired outputs. Instructions, few-shot examples, and structured formats (e.g. \"Answer in one sentence\", \"Step by step\") can...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.8722002506256104, "chunk_id": "machine_learning_3", "text_snippet": "Reinforcement learning (RL) models an agent that takes actions in an environment and receives rewards or penalties. The agent learns a policy that maximizes cumulative reward over time. RL has been su...", "source": "machine_learning.txt"}, {"rank": 4, "score": 1.8819547891616821, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}], "retrieved_sources": ["machine_learning.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2312}}
{"timestamp": "2026-02-16T10:34:06.978719", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "results": [{"rank": 1, "score": 1.7541056871414185, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.7692875862121582, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.8011770248413086, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.8300684690475464, "chunk_id": "machine_learning_4", "text_snippet": "Neural networks are function approximators composed of layers of interconnected nodes (neurons). Each connection has a weight that is learned from data. Activation functions introduce non-linearity (R...", "source": "machine_learning.txt"}], "retrieved_sources": ["llm_concepts.txt", "machine_learning.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2473}}
{"timestamp": "2026-02-16T10:34:07.256078", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "results": [{"rank": 1, "score": 1.7459312677383423, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.7826188802719116, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.8441693782806396, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.8589329719543457, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2768}}
{"timestamp": "2026-02-16T10:36:54.592877", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.6960015296936035, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8462702035903931, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8555042743682861, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8603806495666504, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.396}}
{"timestamp": "2026-02-16T10:36:55.231648", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6260308027267456, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6634612083435059, "chunk_id": "llm_concepts_14", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.7075117826461792, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.74504554271698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.6383}}
{"timestamp": "2026-02-16T10:36:55.519392", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.7347018718719482, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.865385890007019, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9895541667938232, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1145764589309692, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2872}}
{"timestamp": "2026-02-16T10:36:56.101632", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8328152894973755, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.868087887763977, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8998264074325562, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0441789627075195, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.5815}}
{"timestamp": "2026-02-16T10:36:56.342453", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.530610203742981, "chunk_id": "llm_concepts_16", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7539793848991394, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8223064541816711, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8402668237686157, "chunk_id": "llm_practices_11", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2403}}
{"timestamp": "2026-02-16T10:36:56.587545", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.9059703946113586, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9893731474876404, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0064488649368286, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0320048332214355, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2446}}
{"timestamp": "2026-02-16T10:36:56.807623", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8350740671157837, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8576250076293945, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8643407821655273, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8861725330352783, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2195}}
{"timestamp": "2026-02-16T10:36:57.560556", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.8032857179641724, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.114275574684143, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.118534803390503, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1281572580337524, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.7526}}
{"timestamp": "2026-02-16T10:36:57.796292", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9853620529174805, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9971528053283691, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0357369184494019, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1020700931549072, "chunk_id": "llm_concepts_5", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2351}}
{"timestamp": "2026-02-16T10:36:58.021460", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.5392513871192932, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1736438274383545, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2430522441864014, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2915929555892944, "chunk_id": "llm_concepts_24", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": false, "time_sec": 0.2247}}
{"timestamp": "2026-02-16T10:36:58.242241", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.6440763473510742, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6456518769264221, "chunk_id": "llm_practices_9", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8316242694854736, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.866807222366333, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2204}}
{"timestamp": "2026-02-16T10:36:58.491212", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2606570720672607, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842625379562378, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.3589427471160889, "chunk_id": "llm_practices_16", "text_snippet": "depend on exact phrases or entity names, sparse search helps; for conceptual or semantic questions, dense search is better. Fusing the two result lists (e.g. reciprocal rank fusion) gives you a single...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3625268936157227, "chunk_id": "llm_practices_8", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": false, "time_sec": 0.2483}}
{"timestamp": "2026-02-16T10:36:58.712662", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.0910089015960693, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2192013263702393, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2618955373764038, "chunk_id": "llm_concepts_25", "text_snippet": "higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chos...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.221}}
{"timestamp": "2026-02-16T10:36:58.930358", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2702752351760864, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2737611532211304, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.300235629081726, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2168}}
{"timestamp": "2026-02-16T10:36:59.147079", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.9360617399215698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0198891162872314, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0521656274795532, "chunk_id": "llm_practices_21", "text_snippet": "all documents in the prompt; chunking and retrieval select the most relevant subset. Choosing the right chunk size and top-k so that the model receives enough context without overwhelming it is a cent...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.0763764381408691, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": false, "time_sec": 0.2163}}
{"timestamp": "2026-02-16T10:36:59.397663", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "results": [{"rank": 1, "score": 1.8505175113677979, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.8667559623718262, "chunk_id": "llm_concepts_4", "text_snippet": "Prompt engineering is the practice of designing input text (prompts) to get desired outputs. Instructions, few-shot examples, and structured formats (e.g. \"Answer in one sentence\", \"Step by step\") can...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.8722002506256104, "chunk_id": "machine_learning_3", "text_snippet": "Reinforcement learning (RL) models an agent that takes actions in an environment and receives rewards or penalties. The agent learns a policy that maximizes cumulative reward over time. RL has been su...", "source": "machine_learning.txt"}, {"rank": 4, "score": 1.8819547891616821, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}], "retrieved_sources": ["machine_learning.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2502}}
{"timestamp": "2026-02-16T10:37:00.292121", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "results": [{"rank": 1, "score": 1.7541056871414185, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.7692875862121582, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.8011770248413086, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.8300684690475464, "chunk_id": "machine_learning_4", "text_snippet": "Neural networks are function approximators composed of layers of interconnected nodes (neurons). Each connection has a weight that is learned from data. Activation functions introduce non-linearity (R...", "source": "machine_learning.txt"}], "retrieved_sources": ["machine_learning.txt", "llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.8938}}
{"timestamp": "2026-02-16T10:37:00.506479", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "results": [{"rank": 1, "score": 1.7459312677383423, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.7826188802719116, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.8441693782806396, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.8589329719543457, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2139}}
{"timestamp": "2026-02-16T11:09:21.768458", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.6960015296936035, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8462702035903931, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8555042743682861, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8603806495666504, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.5414}}
{"timestamp": "2026-02-16T11:09:22.036192", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6260308027267456, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6634612083435059, "chunk_id": "llm_concepts_14", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.7075117826461792, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.74504554271698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2673}}
{"timestamp": "2026-02-16T11:09:22.316108", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.734548807144165, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8653066158294678, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9895203113555908, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1145915985107422, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2795}}
{"timestamp": "2026-02-16T11:09:22.579852", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8328152894973755, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.868087887763977, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8998264074325562, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0441789627075195, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2625}}
{"timestamp": "2026-02-16T11:09:22.837769", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.530610203742981, "chunk_id": "llm_concepts_16", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7539793848991394, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8223064541816711, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8402668237686157, "chunk_id": "llm_practices_11", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2573}}
{"timestamp": "2026-02-16T11:09:23.064596", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.9059703946113586, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9893731474876404, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0064488649368286, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0320048332214355, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2263}}
{"timestamp": "2026-02-16T11:09:23.279451", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8350740671157837, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8576250076293945, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8643407821655273, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8861725330352783, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2142}}
{"timestamp": "2026-02-16T11:09:23.542745", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.8032857179641724, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.114275574684143, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.118534803390503, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1281572580337524, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2628}}
{"timestamp": "2026-02-16T11:09:23.765531", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9853620529174805, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9971528053283691, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0357369184494019, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1020700931549072, "chunk_id": "llm_concepts_5", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2223}}
{"timestamp": "2026-02-16T11:09:24.011256", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.5392513871192932, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1736438274383545, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2430522441864014, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2915929555892944, "chunk_id": "llm_concepts_24", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.2453}}
{"timestamp": "2026-02-16T11:09:24.235142", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.6440763473510742, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6456518769264221, "chunk_id": "llm_practices_9", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8316242694854736, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.866807222366333, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2234}}
{"timestamp": "2026-02-16T11:09:24.462469", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2606570720672607, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842625379562378, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.3589427471160889, "chunk_id": "llm_practices_16", "text_snippet": "depend on exact phrases or entity names, sparse search helps; for conceptual or semantic questions, dense search is better. Fusing the two result lists (e.g. reciprocal rank fusion) gives you a single...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3625268936157227, "chunk_id": "llm_practices_8", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2268}}
{"timestamp": "2026-02-16T11:09:24.689043", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.0910089015960693, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2192013263702393, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2618955373764038, "chunk_id": "llm_concepts_25", "text_snippet": "higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chos...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2261}}
{"timestamp": "2026-02-16T11:09:24.973545", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2702752351760864, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2737611532211304, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.300235629081726, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.284}}
{"timestamp": "2026-02-16T11:09:25.259349", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.9360617399215698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0198891162872314, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0521656274795532, "chunk_id": "llm_practices_21", "text_snippet": "all documents in the prompt; chunking and retrieval select the most relevant subset. Choosing the right chunk size and top-k so that the model receives enough context without overwhelming it is a cent...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.0763764381408691, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2854}}
{"timestamp": "2026-02-16T11:09:26.896663", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.228, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:09:28.372724", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.243, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:09:29.784731", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2476, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:09:53.875384", "event": "retrieval_eval", "data": {"backend": "query", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.5661568949595326, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.5624106973485679, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.5227201645096521, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.5035012070109387, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2436}}
{"timestamp": "2026-02-16T11:09:54.197731", "event": "retrieval_eval", "data": {"backend": "query", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.5751418449463721, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.5554266090573726, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.5238167881985817, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.5157147171088621, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3219}}
{"timestamp": "2026-02-16T11:09:54.444649", "event": "retrieval_eval", "data": {"backend": "query", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.5133896164382529, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.43252791984051336, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.4141391686416844, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.41355970349919297, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2464}}
{"timestamp": "2026-02-16T11:09:54.711740", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.4502043627340987, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.44892427482130925, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.4349208544308343, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.41998655839217996, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2667}}
{"timestamp": "2026-02-16T11:09:54.949527", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.5712645963804172, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.5600474181403573, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.5450027879825441, "chunk_id": "llm_practices_2", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.539301661237801, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2374}}
{"timestamp": "2026-02-16T11:09:55.167259", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.48722053845209945, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.46930529337947846, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.4590911013579471, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.45610476623296986, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2173}}
{"timestamp": "2026-02-16T11:09:55.489120", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.5546132928035314, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.528676923179718, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.5134882655208862, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.5069751086266173, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3215}}
{"timestamp": "2026-02-16T11:09:55.718855", "event": "retrieval_eval", "data": {"backend": "query", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.5009329833600669, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.3994121592871458, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.38591773434852844, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.37538033288558265, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2293}}
{"timestamp": "2026-02-16T11:09:55.948886", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.4970318463744991, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.4591750062469108, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.43803194556122227, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.4281196440765801, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2296}}
{"timestamp": "2026-02-16T11:09:56.206873", "event": "retrieval_eval", "data": {"backend": "query", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.49849346770877123, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.3331226443059751, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.32667113679414117, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.3227356610224027, "chunk_id": "machine_learning_0", "text_snippet": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can acce...", "source": "machine_learning.txt"}], "retrieved_sources": ["machine_learning.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2575}}
{"timestamp": "2026-02-16T11:09:56.479850", "event": "retrieval_eval", "data": {"backend": "query", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.5566293170837329, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.5523606528288622, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.5306682024843566, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.5298268250021285, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2725}}
{"timestamp": "2026-02-16T11:09:56.696761", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 0.3316961151214666, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.3089683915017665, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.2940204991273256, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.29229830325176165, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2164}}
{"timestamp": "2026-02-16T11:09:56.950077", "event": "retrieval_eval", "data": {"backend": "query", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 0.3932182494787031, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.3122904497407709, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.2491971654022775, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.24469499185901203, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2529}}
{"timestamp": "2026-02-16T11:09:57.205603", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.48373474861542026, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.3207852703748373, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.303692898155118, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.28970261840463896, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2551}}
{"timestamp": "2026-02-16T11:09:57.438588", "event": "retrieval_eval", "data": {"backend": "query", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.46973209260908216, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.4314125499012694, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.4059718788133446, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.3817046112369801, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2326}}
{"timestamp": "2026-02-16T11:10:00.189824", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is the capital of France?", "results": [{"rank": 1, "score": 0.06090740264569302, "chunk_id": "llm_practices_2", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.046817881215352464, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.0419063803892901, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.039642331676959436, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2162, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:10:02.555986", "event": "retrieval_eval", "data": {"backend": "query", "question": "How do you bake a chocolate cake?", "results": [{"rank": 1, "score": 0.0888244576760161, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.06682298077281884, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.06301659981824886, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.05918620233283186, "chunk_id": "llm_practices_2", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2409, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:10:05.732762", "event": "retrieval_eval", "data": {"backend": "query", "question": "What is the population of Tokyo?", "results": [{"rank": 1, "score": 0.08153758058927367, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.04280149443256491, "chunk_id": "llm_practices_2", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.04184803142925848, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.04067126874298195, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.4285, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:10:14.068748", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.6960015296936035, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.845935583114624, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8555042743682861, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8604533672332764, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.411}}
{"timestamp": "2026-02-16T11:10:14.333285", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6260308027267456, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6579909324645996, "chunk_id": "llm_concepts_14", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.7077221274375916, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.74504554271698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2641}}
{"timestamp": "2026-02-16T11:10:14.603797", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.734596848487854, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8653779625892639, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9895203113555908, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1144559383392334, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2699}}
{"timestamp": "2026-02-16T11:10:14.822536", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8328428268432617, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.868087887763977, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8998264074325562, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0441789627075195, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2181}}
{"timestamp": "2026-02-16T11:10:15.050268", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.5305343866348267, "chunk_id": "llm_concepts_16", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7539218068122864, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8221240043640137, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8402941226959229, "chunk_id": "llm_practices_11", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2271}}
{"timestamp": "2026-02-16T11:10:15.271752", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.9059703350067139, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9893732070922852, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0060703754425049, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0320217609405518, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2209}}
{"timestamp": "2026-02-16T11:10:15.485458", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8350740671157837, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8572756052017212, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8645462989807129, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8861725330352783, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.213}}
{"timestamp": "2026-02-16T11:10:15.698264", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.8032724261283875, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.114275574684143, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1185452938079834, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1281483173370361, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2122}}
{"timestamp": "2026-02-16T11:10:15.947219", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9852291345596313, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9971412420272827, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0357550382614136, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1020684242248535, "chunk_id": "llm_concepts_5", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2484}}
{"timestamp": "2026-02-16T11:10:16.172922", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.5392513871192932, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1737817525863647, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2430522441864014, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2915928363800049, "chunk_id": "llm_concepts_24", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.225}}
{"timestamp": "2026-02-16T11:10:16.405465", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.644150972366333, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6456081867218018, "chunk_id": "llm_practices_9", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8316242694854736, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.8661966919898987, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2319}}
{"timestamp": "2026-02-16T11:10:16.676051", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2603691816329956, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842626571655273, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.358922004699707, "chunk_id": "llm_practices_16", "text_snippet": "depend on exact phrases or entity names, sparse search helps; for conceptual or semantic questions, dense search is better. Fusing the two result lists (e.g. reciprocal rank fusion) gives you a single...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.362496018409729, "chunk_id": "llm_practices_8", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2699}}
{"timestamp": "2026-02-16T11:10:16.926443", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.0910089015960693, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2191529273986816, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2622498273849487, "chunk_id": "llm_concepts_25", "text_snippet": "higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chos...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2497}}
{"timestamp": "2026-02-16T11:10:17.141505", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2702752351760864, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.273678183555603, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3002434968948364, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2146}}
{"timestamp": "2026-02-16T11:10:17.388538", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.9360617399215698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.019889235496521, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0520596504211426, "chunk_id": "llm_practices_21", "text_snippet": "all documents in the prompt; chunking and retrieval select the most relevant subset. Choosing the right chunk size and top-k so that the model receives enough context without overwhelming it is a cent...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.0763764381408691, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2466}}
{"timestamp": "2026-02-16T11:10:18.556064", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the capital of France?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2152, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:10:19.756118", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "How do you bake a chocolate cake?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2536, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-16T11:10:22.362195", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the population of Tokyo?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.3083, "generated_answer": "I do not have enough information."}}
{"timestamp": "2026-02-17T02:08:50.697945", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 3.5068, "answer_snippet": "RAG solves the problem of hallucination by grounding answers in retrieved text, allowing the model to cite sources and stay up to date without retraining."}}
{"timestamp": "2026-02-17T02:08:53.875477", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 3.1768, "answer_snippet": "Chunking documents is done to split long texts into retrievable units that fit the retrieval step and the context window. It helps maintain relevant context while ensuring that each chunk is manageabl..."}}
{"timestamp": "2026-02-17T02:08:56.221172", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.345, "answer_snippet": "Embeddings are dense vector representations of text, where each piece of text\u2014such as a sentence or paragraph\u2014is mapped to a fixed-size vector. Similar meanings produce similar vectors, enabling simil..."}}
{"timestamp": "2026-02-17T02:08:57.765436", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.5436, "answer_snippet": "Chunk overlap is used to ensure that sentences are not cut in the middle, preserving surrounding context and improving the precision of retrieval."}}
{"timestamp": "2026-02-17T02:09:01.174883", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 3.409, "answer_snippet": "Top-k in retrieval refers to the value k, which determines how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, the ..."}}
{"timestamp": "2026-02-17T02:09:02.987801", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.8123, "answer_snippet": "RAG (Retrieval-Augmented Generation) can be described in formal terms as modeling p(answer | query, retrieve(query)), where retrieve(query) returns the top-k passages from the corpus."}}
{"timestamp": "2026-02-17T02:09:04.223374", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 1.2351, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T02:09:06.193559", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.9697, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries using the initial retrieved chunks or the model's intermediate answer and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T02:09:08.365866", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 2.1719, "answer_snippet": "Contrastive learning is the typical training objective for embedding models where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T02:09:10.214717", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.8485, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T02:09:11.872300", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.6572, "answer_snippet": "The RAG practices guide recommends a default chunk size of 256 tokens and an overlap of 32 tokens."}}
{"timestamp": "2026-02-17T02:09:13.597161", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.7242, "answer_snippet": "Query rewriting is the process of rephrasing the user question into a more retrieval-friendly form before embedding. It can improve recall, making it easier to retrieve relevant information."}}
{"timestamp": "2026-02-17T02:09:15.583847", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.9861, "answer_snippet": "Use a flat index for latency-sensitive applications with corpora under roughly 100k chunks. Switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T02:09:17.412347", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.828, "answer_snippet": "The incremental indexing rule defines that when you add or update documents, you should only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk ..."}}
{"timestamp": "2026-02-17T02:09:18.997325", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.5844, "answer_snippet": "Storing chunk_id in vector store metadata is important so that the model can reference the correct document and the user can verify the source of the information."}}
{"timestamp": "2026-02-17T02:09:19.987136", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 0.9892, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T02:09:20.935282", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 0.9475, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T02:09:21.893535", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 0.9576, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T02:09:31.740740", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.6960015296936035, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.845935583114624, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8555042743682861, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8604533672332764, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.453}}
{"timestamp": "2026-02-17T02:09:31.954383", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6260308027267456, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6579909324645996, "chunk_id": "llm_concepts_14", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.7077221274375916, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.74504554271698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2133}}
{"timestamp": "2026-02-17T02:09:32.173350", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.734596848487854, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8653779625892639, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9895203113555908, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1144559383392334, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2184}}
{"timestamp": "2026-02-17T02:09:32.409220", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8328428268432617, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.868087887763977, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8998264074325562, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0441789627075195, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2353}}
{"timestamp": "2026-02-17T02:09:32.629022", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.5305343866348267, "chunk_id": "llm_concepts_16", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7539218068122864, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8221240043640137, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8402941226959229, "chunk_id": "llm_practices_11", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2193}}
{"timestamp": "2026-02-17T02:09:32.875587", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.9059703350067139, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9893732070922852, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0060703754425049, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0320217609405518, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2461}}
{"timestamp": "2026-02-17T02:09:33.152180", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8350740671157837, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8572756052017212, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8645462989807129, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8861725330352783, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.276}}
{"timestamp": "2026-02-17T02:09:33.412991", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.8032724261283875, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.114275574684143, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1185452938079834, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1281483173370361, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2602}}
{"timestamp": "2026-02-17T02:09:33.631560", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9852291345596313, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9971412420272827, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0357550382614136, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1020684242248535, "chunk_id": "llm_concepts_5", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.218}}
{"timestamp": "2026-02-17T02:09:33.845542", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.5392513871192932, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1737817525863647, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2430522441864014, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2915928363800049, "chunk_id": "llm_concepts_24", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.2133}}
{"timestamp": "2026-02-17T02:09:34.067651", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.644150972366333, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6456081867218018, "chunk_id": "llm_practices_9", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8316242694854736, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.8661966919898987, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2214}}
{"timestamp": "2026-02-17T02:09:34.334803", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.260318398475647, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842984199523926, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.3588334321975708, "chunk_id": "llm_practices_16", "text_snippet": "depend on exact phrases or entity names, sparse search helps; for conceptual or semantic questions, dense search is better. Fusing the two result lists (e.g. reciprocal rank fusion) gives you a single...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3624961376190186, "chunk_id": "llm_practices_8", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2665}}
{"timestamp": "2026-02-17T02:09:34.734148", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0002903938293457, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.0910131931304932, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2192342281341553, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.262299656867981, "chunk_id": "llm_concepts_25", "text_snippet": "higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chos...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3987}}
{"timestamp": "2026-02-17T02:09:35.040711", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2702752351760864, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.273678183555603, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3002434968948364, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.306}}
{"timestamp": "2026-02-17T02:09:35.300568", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.9360617399215698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.019889235496521, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0520596504211426, "chunk_id": "llm_practices_21", "text_snippet": "all documents in the prompt; chunking and retrieval select the most relevant subset. Choosing the right chunk size and top-k so that the model receives enough context without overwhelming it is a cent...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.0763764381408691, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2593}}
{"timestamp": "2026-02-17T02:09:35.556289", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the capital of France?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2551}}
{"timestamp": "2026-02-17T02:09:35.862501", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "How do you bake a chocolate cake?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.3057}}
{"timestamp": "2026-02-17T02:09:36.101247", "event": "retrieval_eval", "data": {"backend": "faiss", "question": "What is the population of Tokyo?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2381}}
{"timestamp": "2026-02-17T11:52:34.162076", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.7983037233352661, "chunk_id": "llm_practices_9", "text_snippet": "In practice, RAG and fine-tuning address different needs. RAG is best when you have a changing or large knowledge base and want the model to answer from it without retraining. Fine-tuning is best when...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.8611981868743896, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.862047016620636, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8998799920082092, "chunk_id": "llm_concepts_3", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.4173}}
{"timestamp": "2026-02-17T11:52:34.409782", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6431180834770203, "chunk_id": "llm_practices_2", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6927129626274109, "chunk_id": "llm_concepts_6", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8942553997039795, "chunk_id": "llm_concepts_10", "text_snippet": "This paragraph is intentionally off-topic. When building a RAG pipeline, you might ingest documents that contain boilerplate, copyright notices, or unrelated sections. Good preprocessing and chunking ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9364926815032959, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2472}}
{"timestamp": "2026-02-17T11:52:34.656703", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.7766802906990051, "chunk_id": "llm_concepts_5", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8831441402435303, "chunk_id": "llm_practices_1", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1718655824661255, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2315666675567627, "chunk_id": "llm_concepts_11", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2464}}
{"timestamp": "2026-02-17T11:52:34.912830", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8906810879707336, "chunk_id": "llm_practices_2", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.028498649597168, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0454375743865967, "chunk_id": "llm_concepts_6", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1408716440200806, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2557}}
{"timestamp": "2026-02-17T11:52:35.213020", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.60416579246521, "chunk_id": "llm_concepts_7", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7793642282485962, "chunk_id": "llm_practices_5", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8728569746017456, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9325721263885498, "chunk_id": "llm_practices_7", "text_snippet": "\u2022 RAG reduces hallucination by grounding answers in retrieved chunks.\n\u2022 Ask the model to cite chunks; consider verification and human review.\n\u2022 Cleaning input documents can improve retrieval and thus ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2999}}
{"timestamp": "2026-02-17T11:52:35.437764", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 1.0001227855682373, "chunk_id": "llm_practices_9", "text_snippet": "In practice, RAG and fine-tuning address different needs. RAG is best when you have a changing or large knowledge base and want the model to answer from it without retraining. Fine-tuning is best when...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.000195026397705, "chunk_id": "llm_practices_8", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0370783805847168, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.044384479522705, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2243}}
{"timestamp": "2026-02-17T11:52:35.664235", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.7990533113479614, "chunk_id": "llm_practices_9", "text_snippet": "In practice, RAG and fine-tuning address different needs. RAG is best when you have a changing or large knowledge base and want the model to answer from it without retraining. Fine-tuning is best when...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.8847654461860657, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9251013994216919, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9280808568000793, "chunk_id": "llm_practices_8", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.226}}
{"timestamp": "2026-02-17T11:52:35.920347", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.9982999563217163, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0855352878570557, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.164849877357483, "chunk_id": "llm_concepts_3", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1787117719650269, "chunk_id": "llm_concepts_7", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.2556}}
{"timestamp": "2026-02-17T11:52:36.234818", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9603841304779053, "chunk_id": "llm_concepts_5", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0058780908584595, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0338053703308105, "chunk_id": "llm_practices_1", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.087310552597046, "chunk_id": "llm_concepts_2", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.314}}
{"timestamp": "2026-02-17T11:52:36.493259", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.8788719773292542, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.2984271049499512, "chunk_id": "llm_concepts_11", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.3201960325241089, "chunk_id": "llm_concepts_2", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3342782258987427, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.258}}
{"timestamp": "2026-02-17T11:52:36.848534", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.6414488554000854, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.8000329732894897, "chunk_id": "llm_practices_2", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8003292083740234, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8363180160522461, "chunk_id": "llm_practices_9", "text_snippet": "In practice, RAG and fine-tuning address different needs. RAG is best when you have a changing or large knowledge base and want the model to answer from it without retraining. Fine-tuning is best when...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.3548}}
{"timestamp": "2026-02-17T11:52:37.113897", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2459241151809692, "chunk_id": "llm_practices_7", "text_snippet": "\u2022 RAG reduces hallucination by grounding answers in retrieved chunks.\n\u2022 Ask the model to cite chunks; consider verification and human review.\n\u2022 Cleaning input documents can improve retrieval and thus ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842625379562378, "chunk_id": "llm_practices_11", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.3823955059051514, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3844127655029297, "chunk_id": "llm_concepts_3", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2649}}
{"timestamp": "2026-02-17T11:52:37.356777", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_11", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2951699495315552, "chunk_id": "llm_concepts_5", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.3731310367584229, "chunk_id": "llm_practices_1", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.4645582437515259, "chunk_id": "llm_practices_7", "text_snippet": "\u2022 RAG reduces hallucination by grounding answers in retrieved chunks.\n\u2022 Ask the model to cite chunks; consider verification and human review.\n\u2022 Cleaning input documents can improve retrieval and thus ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2424}}
{"timestamp": "2026-02-17T11:52:37.608617", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_11", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3592984676361084, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.361822485923767, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3815557956695557, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2514}}
{"timestamp": "2026-02-17T11:52:37.859059", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 1.1320381164550781, "chunk_id": "llm_practices_1", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.1538238525390625, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.187307357788086, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.20559561252594, "chunk_id": "llm_practices_2", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.25}}
{"timestamp": "2026-02-17T11:52:38.155462", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2959}}
{"timestamp": "2026-02-17T11:52:38.388275", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2325}}
{"timestamp": "2026-02-17T11:52:38.692075", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.3034}}
{"timestamp": "2026-02-17T11:52:42.279045", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.2863, "answer_snippet": "RAG solves the problem of keeping knowledge up to date without retraining the model and reducing hallucination by grounding answers in retrieved text."}}
{"timestamp": "2026-02-17T11:52:44.834794", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 2.5553, "answer_snippet": "Chunking documents is done to split long documents into pieces that fit the retrieval step and the context window. It helps maintain semantic units, reduces the chance of losing surrounding context, a..."}}
{"timestamp": "2026-02-17T11:52:47.221373", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.3861, "answer_snippet": "Embeddings are dense vector representations of text, where each piece of text is mapped to a fixed-size vector. Similar meanings produce similar vectors, enabling similarity search in vector space to ..."}}
{"timestamp": "2026-02-17T11:52:49.514796", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 2.2928, "answer_snippet": "Chunk overlap is used to reduce boundary effects when splitting documents into chunks. It helps maintain surrounding context and ensures that sentences are not cut in the middle, which can improve the..."}}
{"timestamp": "2026-02-17T11:52:52.488525", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.9732, "answer_snippet": "Top-k in retrieval refers to the parameter k that determines how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, th..."}}
{"timestamp": "2026-02-17T11:52:53.866411", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": [], "hit": false, "time_sec": 1.3774, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:52:55.277265", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 1.4103, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:52:57.210471", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.9327, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries (using the initial retrieved chunks or the model's intermediate answer) and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T11:52:59.067491", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.8565, "answer_snippet": "Contrastive learning is the typical training objective for embedding models where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T11:53:00.855199", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.7872, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T11:53:02.499308", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.6437, "answer_snippet": "The RAG practices guide recommends a default chunk size of 256 tokens and an overlap of 32 tokens."}}
{"timestamp": "2026-02-17T11:53:04.667214", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": ["llm_practices.txt", "llm_practices_11.txt"], "hit": true, "time_sec": 2.1673, "answer_snippet": "Query rewriting is the process of rephrasing the user question into a more retrieval-friendly form before embedding. It is used to improve recall, which can enhance the effectiveness of the retrieval ..."}}
{"timestamp": "2026-02-17T11:53:06.148973", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.4812, "answer_snippet": "Use a flat index (exact search) for latency-sensitive applications with corpora under roughly 100k chunks. Switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T11:53:07.782232", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.6327, "answer_snippet": "The incremental indexing rule defines that when you add or update documents, only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk IDs are sta..."}}
{"timestamp": "2026-02-17T11:53:09.371936", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.5892, "answer_snippet": "You should store chunk_id in vector store metadata so the model can cite specific chunks, allowing users to check which chunks supported the answer."}}
{"timestamp": "2026-02-17T11:53:10.527777", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 1.1553, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:53:11.664252", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 1.1359, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:53:12.820588", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 1.1558, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:53:14.661192", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.8676861524581909, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.8747826814651489, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9547345638275146, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9929119944572449, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3756}}
{"timestamp": "2026-02-17T11:53:14.924829", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.8498893976211548, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8881575465202332, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9517967104911804, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.968418300151825, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2632}}
{"timestamp": "2026-02-17T11:53:15.172665", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.9735124111175537, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.134939432144165, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1717355251312256, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.172817349433899, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2474}}
{"timestamp": "2026-02-17T11:53:15.400816", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 1.099091649055481, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.102043867111206, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.1301696300506592, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.160104751586914, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2278}}
{"timestamp": "2026-02-17T11:53:15.656450", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.8572636842727661, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8801422119140625, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.9099944233894348, "chunk_id": "llm_practices_2", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.9214882850646973, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2552}}
{"timestamp": "2026-02-17T11:53:15.893819", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 1.0261214971542358, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.061389446258545, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0819519758224487, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0876665115356445, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2369}}
{"timestamp": "2026-02-17T11:53:16.118313", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8908637762069702, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.9426461458206177, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9729955196380615, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9861631393432617, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2241}}
{"timestamp": "2026-02-17T11:53:16.477231", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.9981955289840698, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.2010215520858765, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.228154182434082, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2492189407348633, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3584}}
{"timestamp": "2026-02-17T11:53:16.711436", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 1.0059945583343506, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0817798376083374, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.1226531267166138, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1438778638839722, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2337}}
{"timestamp": "2026-02-17T11:53:16.987764", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 1.0037956237792969, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.3337920904159546, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.3458967208862305, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3545286655426025, "chunk_id": "machine_learning_0", "text_snippet": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can acce...", "source": "machine_learning.txt"}], "retrieved_sources": ["llm_concepts.txt", "machine_learning.txt"], "hit": true, "time_sec": 0.2758}}
{"timestamp": "2026-02-17T11:53:17.224362", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.8867570757865906, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.895190417766571, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.937854528427124, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.9402251243591309, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2362}}
{"timestamp": "2026-02-17T11:53:17.448477", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.3361999988555908, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3820767402648926, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.4117034673690796, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.4162814617156982, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2236}}
{"timestamp": "2026-02-17T11:53:17.808464", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.213672399520874, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3758494853973389, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3595}}
{"timestamp": "2026-02-17T11:53:18.061726", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 1.032606601715088, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3584580421447754, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.39247727394104, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.420637845993042, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2527}}
{"timestamp": "2026-02-17T11:53:18.319770", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 1.0612521171569824, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1369178295135498, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1883459091186523, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2369951009750366, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2576}}
{"timestamp": "2026-02-17T11:53:18.630510", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.3102}}
{"timestamp": "2026-02-17T11:53:18.887801", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2569}}
{"timestamp": "2026-02-17T11:53:19.118173", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.23}}
{"timestamp": "2026-02-17T11:53:23.270237", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 2.736, "answer_snippet": "RAG solves the problem of hallucination in language models by grounding answers in retrieved text, allowing the model to answer from a changing or large knowledge base without the need for retraining."}}
{"timestamp": "2026-02-17T11:53:25.485947", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.2152, "answer_snippet": "We chunk documents to split long texts into retrievable units, which helps improve the precision and relevance of information retrieval. Chunking allows for better management of context and ensures th..."}}
{"timestamp": "2026-02-17T11:53:29.070018", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 3.5835, "answer_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector. Similar meanings produce similar vectors, allowing..."}}
{"timestamp": "2026-02-17T11:53:33.370760", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 4.3001, "answer_snippet": "Chunk overlap is used to reduce boundary effects when splitting documents into chunks, helping to avoid cutting sentences or concepts in half. It ensures that relevant context is preserved across adja..."}}
{"timestamp": "2026-02-17T11:53:36.340487", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.9691, "answer_snippet": "Top-k in retrieval refers to the parameter that controls how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, the fi..."}}
{"timestamp": "2026-02-17T11:53:37.465160", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": [], "hit": false, "time_sec": 1.1241, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:53:38.654282", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 1.1886, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:53:40.234905", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.5801, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries using the initial retrieved chunks or the model's intermediate answer and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T11:53:42.177251", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.9418, "answer_snippet": "Contrastive learning is the typical training objective for embedding models where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T11:53:44.445620", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 2.268, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T11:53:46.170476", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.7243, "answer_snippet": "The RAG practices guide recommends experimenting with chunk sizes of 200\u2013500 tokens and an overlap of 20\u201350 tokens."}}
{"timestamp": "2026-02-17T11:53:48.044032", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.8731, "answer_snippet": "Query rewriting is the process of rephrasing the user question into a more retrieval-friendly form before embedding it. This technique can improve recall, allowing for better retrieval of relevant chu..."}}
{"timestamp": "2026-02-17T11:53:49.652448", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.6079, "answer_snippet": "Use a flat index for latency-sensitive applications with corpora under roughly 100k chunks. Switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T11:53:51.168929", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.516, "answer_snippet": "The incremental indexing rule states that when you add or update documents, you should only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk I..."}}
{"timestamp": "2026-02-17T11:53:53.045934", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.8766, "answer_snippet": "Storing chunk_id in vector store metadata is important so that the model can reference the correct document and the user can verify the information provided. This allows for accurate citations and ens..."}}
{"timestamp": "2026-02-17T11:53:54.065433", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 1.0189, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:53:54.990834", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 0.9249, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:53:55.904584", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 0.9132, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:53:57.866779", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.9097669124603271, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9298489093780518, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9743603467941284, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.045147180557251, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.4876}}
{"timestamp": "2026-02-17T11:53:58.101186", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.9812992215156555, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.9834392666816711, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0596494674682617, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1843620538711548, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2341}}
{"timestamp": "2026-02-17T11:53:58.670413", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 1.1718099117279053, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.2702981233596802, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.33048677444458, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3305425643920898, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.5688}}
{"timestamp": "2026-02-17T11:53:58.974397", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 1.1365957260131836, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.1446807384490967, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1533852815628052, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2140055894851685, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3033}}
{"timestamp": "2026-02-17T11:53:59.206904", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.9588179588317871, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0168321132659912, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.074376106262207, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.080352544784546, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2319}}
{"timestamp": "2026-02-17T11:53:59.429742", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 1.0613429546356201, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0984630584716797, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.10073983669281, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1852625608444214, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2224}}
{"timestamp": "2026-02-17T11:53:59.683024", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.9403129816055298, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.9816773533821106, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.9996827840805054, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.062436580657959, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2527}}
{"timestamp": "2026-02-17T11:53:59.906594", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.9982999563217163, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.2636770009994507, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.263858437538147, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.288790225982666, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2231}}
{"timestamp": "2026-02-17T11:54:00.209616", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 1.0058780908584595, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1461427211761475, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2816566228866577, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.31123948097229, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3026}}
{"timestamp": "2026-02-17T11:54:00.509139", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 1.1553151607513428, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.3342782258987427, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.426542043685913, "chunk_id": "machine_learning_0", "text_snippet": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can acce...", "source": "machine_learning.txt"}, {"rank": 4, "score": 1.4669859409332275, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "machine_learning.txt"], "hit": true, "time_sec": 0.2991}}
{"timestamp": "2026-02-17T11:54:00.817439", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.7659802436828613, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.767387866973877, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9696542024612427, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.973135232925415, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3078}}
{"timestamp": "2026-02-17T11:54:01.044001", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.3823955059051514, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.4610199928283691, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.4630115032196045, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.4699788093566895, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": false, "time_sec": 0.2262}}
{"timestamp": "2026-02-17T11:54:01.329690", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.4576796293258667, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2852}}
{"timestamp": "2026-02-17T11:54:01.638193", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 1.2120544910430908, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3592984676361084, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.4204500913619995, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.4451996088027954, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3081}}
{"timestamp": "2026-02-17T11:54:01.942163", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 1.1906582117080688, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.1928040981292725, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.2388255596160889, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3605507612228394, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3035}}
{"timestamp": "2026-02-17T11:54:02.195161", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2525}}
{"timestamp": "2026-02-17T11:54:02.630719", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.4351}}
{"timestamp": "2026-02-17T11:54:02.859895", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2287}}
{"timestamp": "2026-02-17T11:54:07.973667", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 3.7208, "answer_snippet": "RAG solves the problem of hallucination in language models by grounding answers in retrieved text, allowing the model to cite sources and stay up to date without needing to retrain on a changing or la..."}}
{"timestamp": "2026-02-17T11:54:10.030090", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 2.056, "answer_snippet": "We chunk documents to split long texts into retrievable units that fit the retrieval step and the context window. This helps maintain context while ensuring that the chunks are manageable for embeddin..."}}
{"timestamp": "2026-02-17T11:54:12.276325", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.2456, "answer_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector. Similar meanings produce similar vectors, allowing..."}}
{"timestamp": "2026-02-17T11:54:14.140433", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.8635, "answer_snippet": "Chunk overlap is used to reduce boundary effects when splitting documents into chunks. It helps ensure that sentences or concepts are not cut off in the middle, thereby preserving context and improvin..."}}
{"timestamp": "2026-02-17T11:54:17.472416", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 3.3314, "answer_snippet": "Top-k in retrieval refers to the parameter that controls how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, the fi..."}}
{"timestamp": "2026-02-17T11:54:18.630979", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": [], "hit": false, "time_sec": 1.1581, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:54:19.962773", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 1.3312, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:54:22.361162", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 2.3978, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries using the initial retrieved chunks or the model's intermediate answer and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T11:54:23.741871", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.3803, "answer_snippet": "Contrastive learning is the typical training objective for embedding models where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T11:54:25.290869", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.5485, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T11:54:26.926132", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.6347, "answer_snippet": "The RAG practices guide recommends a default chunk size of 256 tokens and an overlap of 32 tokens."}}
{"timestamp": "2026-02-17T11:54:27.959642", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": [], "hit": false, "time_sec": 1.033, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:54:29.588436", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.6282, "answer_snippet": "You should use a flat index (exact search) for corpora under roughly 100k chunks and switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T11:54:33.683405", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 4.0944, "answer_snippet": "The incremental indexing rule states that when you add or update documents, only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk IDs are stab..."}}
{"timestamp": "2026-02-17T11:54:36.209699", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 2.526, "answer_snippet": "You should store chunk_id in vector store metadata so that the model can cite the correct document and the user can verify which chunks supported the answer."}}
{"timestamp": "2026-02-17T11:54:37.376287", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 1.1663, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:54:38.459639", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 1.0827, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:54:39.419012", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 0.9588, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:55:17.230340", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.6960015296936035, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8462702035903931, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8555042743682861, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8603806495666504, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.4367}}
{"timestamp": "2026-02-17T11:55:17.481011", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6260308027267456, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6634612083435059, "chunk_id": "llm_concepts_14", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.7075117826461792, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.74504554271698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2503}}
{"timestamp": "2026-02-17T11:55:17.709786", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.734548807144165, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8653066158294678, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9895203113555908, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1145915985107422, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2283}}
{"timestamp": "2026-02-17T11:55:17.964978", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8328152894973755, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.868087887763977, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8998264074325562, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0441789627075195, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2547}}
{"timestamp": "2026-02-17T11:55:18.195221", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.530610203742981, "chunk_id": "llm_concepts_16", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7539793848991394, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8223064541816711, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8402668237686157, "chunk_id": "llm_practices_11", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2298}}
{"timestamp": "2026-02-17T11:55:18.419584", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.9059703946113586, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9893731474876404, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0064488649368286, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0320048332214355, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2238}}
{"timestamp": "2026-02-17T11:55:18.743368", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8350740671157837, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8576250076293945, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8643407821655273, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8861725330352783, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3233}}
{"timestamp": "2026-02-17T11:55:19.051850", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.8032605648040771, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1140861511230469, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1184182167053223, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1280605792999268, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.308}}
{"timestamp": "2026-02-17T11:55:19.358603", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9853620529174805, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9971528053283691, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0357369184494019, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1020700931549072, "chunk_id": "llm_concepts_5", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3063}}
{"timestamp": "2026-02-17T11:55:19.664851", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.5391084551811218, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1732792854309082, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2428746223449707, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.291393518447876, "chunk_id": "llm_concepts_24", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.3057}}
{"timestamp": "2026-02-17T11:55:19.892805", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.6440763473510742, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6456518769264221, "chunk_id": "llm_practices_9", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8316242694854736, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.866807222366333, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2275}}
{"timestamp": "2026-02-17T11:55:20.118801", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2606570720672607, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842625379562378, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.3589427471160889, "chunk_id": "llm_practices_16", "text_snippet": "depend on exact phrases or entity names, sparse search helps; for conceptual or semantic questions, dense search is better. Fusing the two result lists (e.g. reciprocal rank fusion) gives you a single...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3625268936157227, "chunk_id": "llm_practices_8", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2255}}
{"timestamp": "2026-02-17T11:55:20.387866", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.0910089015960693, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2192013263702393, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2618955373764038, "chunk_id": "llm_concepts_25", "text_snippet": "higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chos...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2686}}
{"timestamp": "2026-02-17T11:55:20.689050", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8222938776016235, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2704064846038818, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2739269733428955, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.300297737121582, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3007}}
{"timestamp": "2026-02-17T11:55:20.996376", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.9360617399215698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0198891162872314, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0521656274795532, "chunk_id": "llm_practices_21", "text_snippet": "all documents in the prompt; chunking and retrieval select the most relevant subset. Choosing the right chunk size and top-k so that the model receives enough context without overwhelming it is a cent...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.0763764381408691, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3068}}
{"timestamp": "2026-02-17T11:55:21.244682", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2478}}
{"timestamp": "2026-02-17T11:55:21.459110", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2141}}
{"timestamp": "2026-02-17T11:55:21.714201", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "results": [], "retrieved_sources": [], "hit": true, "time_sec": 0.2547}}
{"timestamp": "2026-02-17T11:55:34.121478", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.556, "answer_snippet": "RAG solves the problem of hallucination by grounding answers in retrieved text, allowing the model to cite sources and stay up to date without the need for retraining."}}
{"timestamp": "2026-02-17T11:55:35.955625", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.8336, "answer_snippet": "Chunking documents is done to split long texts into retrievable units that fit the retrieval step and the context window. It helps maintain surrounding context and improves the precision of retrieval ..."}}
{"timestamp": "2026-02-17T11:55:37.745449", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 1.7892, "answer_snippet": "Embeddings are dense vector representations of text, where each piece of text is mapped to a fixed-size vector. Similar meanings produce similar vectors, allowing for similarity search in vector space..."}}
{"timestamp": "2026-02-17T11:55:39.426757", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.6807, "answer_snippet": "Chunk overlap is used to ensure that sentences are not cut in the middle, preserving surrounding context and improving the precision of retrieval."}}
{"timestamp": "2026-02-17T11:55:41.781515", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.3541, "answer_snippet": "Top-k in retrieval refers to the value k that determines how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, the fi..."}}
{"timestamp": "2026-02-17T11:55:43.727630", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.9455, "answer_snippet": "RAG (Retrieval-Augmented Generation) can be described formally as modeling p(answer | query, retrieve(query)), where retrieve(query) returns the top-k passages from the corpus."}}
{"timestamp": "2026-02-17T11:55:44.750983", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 1.0227, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:55:46.308516", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.557, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries using the initial retrieved chunks or the model's intermediate answer and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T11:55:48.314941", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 2.0061, "answer_snippet": "Contrastive learning is the typical training objective for embedding models where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T11:55:49.768643", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.4534, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T11:55:51.079687", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.3105, "answer_snippet": "The RAG practices guide recommends a default chunk size of 256 tokens and an overlap of 32."}}
{"timestamp": "2026-02-17T11:55:53.122839", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 2.0426, "answer_snippet": "Query rewriting is the process of rephrasing the user question into a more retrieval-friendly form before embedding. It can improve recall, making it easier to retrieve relevant information."}}
{"timestamp": "2026-02-17T11:55:54.683922", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.5605, "answer_snippet": "Use a flat index for latency-sensitive applications when the corpus is under roughly 100k chunks. Switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T11:55:56.423465", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.739, "answer_snippet": "The incremental indexing rule defines that when you add or update documents, you should only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk ..."}}
{"timestamp": "2026-02-17T11:55:57.921756", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.4979, "answer_snippet": "Storing chunk_id in vector store metadata is important so that the model can reference the correct document and the user can verify the source of the information."}}
{"timestamp": "2026-02-17T11:55:59.292185", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 1.3698, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:56:00.520848", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 1.2281, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T11:56:01.546392", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 1.0249, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:27:51.433925", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.095, "answer_snippet": "RAG solves the problem of hallucination by grounding answers in retrieved text, allowing the model to cite sources and stay up to date without retraining."}}
{"timestamp": "2026-02-17T12:27:53.969835", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 2.5321, "answer_snippet": "Chunking documents is done to split long texts into retrievable units that fit the retrieval step and the context window. It helps maintain surrounding context, ensures coherent themes, and improves t..."}}
{"timestamp": "2026-02-17T12:27:56.381506", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.411, "answer_snippet": "Embeddings are dense vector representations of text, where each piece of text is mapped to a fixed-size vector. Similar meanings produce similar vectors, allowing for similarity search in vector space..."}}
{"timestamp": "2026-02-17T12:27:57.987274", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.6052, "answer_snippet": "Chunk overlap is used to ensure that sentences are not cut in the middle, preserving surrounding context and improving the precision of retrieval."}}
{"timestamp": "2026-02-17T12:28:00.976617", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.989, "answer_snippet": "Top-k in retrieval refers to the value k that determines how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, the fi..."}}
{"timestamp": "2026-02-17T12:28:02.408418", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.4312, "answer_snippet": "RAG (Retrieval-Augmented Generation) can be described in formal terms as modeling p(answer | query, retrieve(query)), where retrieve(query) returns the top-k passages from the corpus."}}
{"timestamp": "2026-02-17T12:28:03.339800", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 0.9308, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:28:04.812009", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.4717, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries using the initial retrieved chunks or the model's intermediate answer and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T12:28:06.480465", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.6678, "answer_snippet": "Contrastive learning is the typical training objective for embedding models where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T12:28:07.799323", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.3183, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T12:28:09.009289", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.2095, "answer_snippet": "The RAG practices guide recommends a default chunk size of 256 tokens and an overlap of 32 tokens."}}
{"timestamp": "2026-02-17T12:28:10.520217", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.5103, "answer_snippet": "Query rewriting is the process of rephrasing the user question into a more retrieval-friendly form before embedding. It can improve recall, making it easier to retrieve relevant information."}}
{"timestamp": "2026-02-17T12:28:13.196138", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 2.6753, "answer_snippet": "You should use a flat index for latency-sensitive applications with corpora under roughly 100k chunks. Switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T12:28:14.774004", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.5774, "answer_snippet": "The incremental indexing rule defines that when you add or update documents, you should only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk ..."}}
{"timestamp": "2026-02-17T12:28:16.412218", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.6376, "answer_snippet": "Storing chunk_id in vector store metadata is important so that the model can reference the correct document and the user can verify the source of the information."}}
{"timestamp": "2026-02-17T12:28:17.328511", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 0.9157, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:28:18.392709", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 1.0636, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:28:19.364070", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 0.9707, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:31:54.855357", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.6960015296936035, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8462702035903931, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8555042743682861, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8603806495666504, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.4047}}
{"timestamp": "2026-02-17T12:31:55.223354", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6260308027267456, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6634612083435059, "chunk_id": "llm_concepts_14", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.7075117826461792, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.74504554271698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3676}}
{"timestamp": "2026-02-17T12:31:55.479542", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.734548807144165, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8653066158294678, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9895203113555908, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1145915985107422, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2557}}
{"timestamp": "2026-02-17T12:31:55.699435", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8328152894973755, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.868087887763977, "chunk_id": "llm_practices_5", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8998264074325562, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0441789627075195, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2194}}
{"timestamp": "2026-02-17T12:31:55.918604", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.530610203742981, "chunk_id": "llm_concepts_16", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7539793848991394, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8223064541816711, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8402668237686157, "chunk_id": "llm_practices_11", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2187}}
{"timestamp": "2026-02-17T12:31:56.264028", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 0.9059703946113586, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9893731474876404, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0064488649368286, "chunk_id": "llm_concepts_7", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0320048332214355, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.345}}
{"timestamp": "2026-02-17T12:31:56.680363", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8350740671157837, "chunk_id": "llm_concepts_18", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8576250076293945, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8643407821655273, "chunk_id": "llm_practices_23", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8861725330352783, "chunk_id": "llm_practices_17", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.4159}}
{"timestamp": "2026-02-17T12:31:56.896859", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.8032857179641724, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.114275574684143, "chunk_id": "llm_practices_12", "text_snippet": "or k=3 is often sufficient. For questions that require combining information from several places, k=5 to k=10 or multi-hop retrieval may be needed. Validating on a set of held-out questions and measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.118534803390503, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1281572580337524, "chunk_id": "llm_concepts_20", "text_snippet": "After retrieving a top-k set with cosine or dot-product similarity, you can re-rank the candidates using a cross-encoder or a more expensive model. Re-ranking improves precision: the initial retriever...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.216}}
{"timestamp": "2026-02-17T12:31:57.139418", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9853620529174805, "chunk_id": "llm_concepts_27", "text_snippet": "Exclusive to this file (LLM Concepts only): the \"information bottleneck\" in RAG is the retriever\u2014the full corpus is never seen by the LLM, only the top-k chunks are. Thus retrieval quality upper-bound...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9971528053283691, "chunk_id": "llm_concepts_11", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0357369184494019, "chunk_id": "llm_practices_2", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1020700931549072, "chunk_id": "llm_concepts_5", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2421}}
{"timestamp": "2026-02-17T12:31:57.361991", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.5392513871192932, "chunk_id": "llm_concepts_1", "text_snippet": "The transformer architecture, introduced in \"Attention Is All You Need\" (2017), replaced recurrent layers with self-attention. Each token in a sequence can attend to every other token, so the model ca...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1736438274383545, "chunk_id": "llm_concepts_2", "text_snippet": "Transformers are highly parallelizable, which enabled training on much larger datasets and models. The encoder-decoder design is used for sequence-to-sequence tasks (e.g. translation); decoder-only mo...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2430522441864014, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2915929555892944, "chunk_id": "llm_concepts_24", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.2222}}
{"timestamp": "2026-02-17T12:31:57.587590", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.6440763473510742, "chunk_id": "llm_practices_6", "text_snippet": "(split when the embedding changes sharply), or by document structure (e.g. sections under headings). There is no universal best chunk size; it depends on your document length and query style. Experime...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6456518769264221, "chunk_id": "llm_practices_9", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8316242694854736, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.866807222366333, "chunk_id": "llm_practices_22", "text_snippet": "This section is off-topic for RAG design. Sometimes ingested files contain version history, change logs, or legal disclaimers. Your chunking and cleaning steps should aim to exclude or downweight such...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2251}}
{"timestamp": "2026-02-17T12:31:57.805871", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2606570720672607, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842625379562378, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.3589427471160889, "chunk_id": "llm_practices_16", "text_snippet": "depend on exact phrases or entity names, sparse search helps; for conceptual or semantic questions, dense search is better. Fusing the two result lists (e.g. reciprocal rank fusion) gives you a single...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3625268936157227, "chunk_id": "llm_practices_8", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2179}}
{"timestamp": "2026-02-17T12:31:58.019391", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.0910089015960693, "chunk_id": "llm_concepts_12", "text_snippet": "(e.g. L2) is often applied so that cosine similarity equals the dot product, which simplifies implementation. Vector stores index these embeddings for fast retrieval. Options include ChromaDB, FAISS, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2192013263702393, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2618955373764038, "chunk_id": "llm_concepts_25", "text_snippet": "higher k gives more context but more noise, and it is best to tune on validation queries. Embeddings and vector stores should use the same model for indexing and querying, and the store should be chos...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.213}}
{"timestamp": "2026-02-17T12:31:58.239829", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_25", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2702752351760864, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2737611532211304, "chunk_id": "llm_practices_4", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.300235629081726, "chunk_id": "llm_practices_15", "text_snippet": "Re-ranking is an optional step after initial retrieval. Instead of sending all top-k chunks to the LLM, you can run a cross-encoder or another model over the query and each candidate chunk to score th...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.22}}
{"timestamp": "2026-02-17T12:31:58.498575", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 0.9360617399215698, "chunk_id": "llm_concepts_9", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0198891162872314, "chunk_id": "llm_concepts_13", "text_snippet": "metadata such as source or date are important for production RAG. It is important to use the same embedding model at index time and at query time to avoid dimension mismatch. Chunk size matters a grea...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0521656274795532, "chunk_id": "llm_practices_21", "text_snippet": "all documents in the prompt; chunking and retrieval select the most relevant subset. Choosing the right chunk size and top-k so that the model receives enough context without overwhelming it is a cent...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.0763764381408691, "chunk_id": "llm_concepts_19", "text_snippet": "document chunking, embedding model, vector store, and retrieval and prompt assembly. Chunk size and overlap affect quality. When you run a RAG tutorial, trying different chunk sizes and different k va...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2583}}
{"timestamp": "2026-02-17T12:32:19.921037", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.7983037233352661, "chunk_id": "llm_practices_9", "text_snippet": "In practice, RAG and fine-tuning address different needs. RAG is best when you have a changing or large knowledge base and want the model to answer from it without retraining. Fine-tuning is best when...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.8611981868743896, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.862047016620636, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8998799920082092, "chunk_id": "llm_concepts_3", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.419}}
{"timestamp": "2026-02-17T12:32:20.141867", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6431180834770203, "chunk_id": "llm_practices_2", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6927129626274109, "chunk_id": "llm_concepts_6", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8942553997039795, "chunk_id": "llm_concepts_10", "text_snippet": "This paragraph is intentionally off-topic. When building a RAG pipeline, you might ingest documents that contain boilerplate, copyright notices, or unrelated sections. Good preprocessing and chunking ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9364926815032959, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2205}}
{"timestamp": "2026-02-17T12:32:20.391404", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.7766802906990051, "chunk_id": "llm_concepts_5", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8831441402435303, "chunk_id": "llm_practices_1", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1718655824661255, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2315666675567627, "chunk_id": "llm_concepts_11", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2491}}
{"timestamp": "2026-02-17T12:32:20.644616", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8906810879707336, "chunk_id": "llm_practices_2", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.028498649597168, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0454375743865967, "chunk_id": "llm_concepts_6", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1408716440200806, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2528}}
{"timestamp": "2026-02-17T12:32:20.898814", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.60416579246521, "chunk_id": "llm_concepts_7", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7793642282485962, "chunk_id": "llm_practices_5", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8728569746017456, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9325721263885498, "chunk_id": "llm_practices_7", "text_snippet": "\u2022 RAG reduces hallucination by grounding answers in retrieved chunks.\n\u2022 Ask the model to cite chunks; consider verification and human review.\n\u2022 Cleaning input documents can improve retrieval and thus ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2538}}
{"timestamp": "2026-02-17T12:32:21.152555", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 1.0001227855682373, "chunk_id": "llm_practices_9", "text_snippet": "In practice, RAG and fine-tuning address different needs. RAG is best when you have a changing or large knowledge base and want the model to answer from it without retraining. Fine-tuning is best when...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.000195026397705, "chunk_id": "llm_practices_8", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0370783805847168, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.044384479522705, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2531}}
{"timestamp": "2026-02-17T12:32:21.413015", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.7990533113479614, "chunk_id": "llm_practices_9", "text_snippet": "In practice, RAG and fine-tuning address different needs. RAG is best when you have a changing or large knowledge base and want the model to answer from it without retraining. Fine-tuning is best when...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.8847654461860657, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9251013994216919, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9280808568000793, "chunk_id": "llm_practices_8", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.26}}
{"timestamp": "2026-02-17T12:32:21.652288", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.9982999563217163, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0855352878570557, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.164849877357483, "chunk_id": "llm_concepts_3", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1787117719650269, "chunk_id": "llm_concepts_7", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.2388}}
{"timestamp": "2026-02-17T12:32:21.899175", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9603841304779053, "chunk_id": "llm_concepts_5", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0058780908584595, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0338053703308105, "chunk_id": "llm_practices_1", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.087310552597046, "chunk_id": "llm_concepts_2", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2464}}
{"timestamp": "2026-02-17T12:32:22.163420", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.8788719773292542, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.2984271049499512, "chunk_id": "llm_concepts_11", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.3201960325241089, "chunk_id": "llm_concepts_2", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3342782258987427, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.2638}}
{"timestamp": "2026-02-17T12:32:22.422645", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.6414488554000854, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.8000329732894897, "chunk_id": "llm_practices_2", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8003292083740234, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8363180160522461, "chunk_id": "llm_practices_9", "text_snippet": "In practice, RAG and fine-tuning address different needs. RAG is best when you have a changing or large knowledge base and want the model to answer from it without retraining. Fine-tuning is best when...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2589}}
{"timestamp": "2026-02-17T12:32:22.659761", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2459241151809692, "chunk_id": "llm_practices_7", "text_snippet": "\u2022 RAG reduces hallucination by grounding answers in retrieved chunks.\n\u2022 Ask the model to cite chunks; consider verification and human review.\n\u2022 Cleaning input documents can improve retrieval and thus ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842625379562378, "chunk_id": "llm_practices_11", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.3823955059051514, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3844127655029297, "chunk_id": "llm_concepts_3", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2367}}
{"timestamp": "2026-02-17T12:32:22.928755", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_11", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2951699495315552, "chunk_id": "llm_concepts_5", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.3731310367584229, "chunk_id": "llm_practices_1", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.4645582437515259, "chunk_id": "llm_practices_7", "text_snippet": "\u2022 RAG reduces hallucination by grounding answers in retrieved chunks.\n\u2022 Ask the model to cite chunks; consider verification and human review.\n\u2022 Cleaning input documents can improve retrieval and thus ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2685}}
{"timestamp": "2026-02-17T12:32:23.147923", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_11", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3592984676361084, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.361822485923767, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3815557956695557, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2187}}
{"timestamp": "2026-02-17T12:32:23.361559", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 1.1320381164550781, "chunk_id": "llm_practices_1", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.1538238525390625, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.187307357788086, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.20559561252594, "chunk_id": "llm_practices_2", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2132}}
{"timestamp": "2026-02-17T12:32:26.454673", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 1.6797, "answer_snippet": "RAG solves the problem of keeping knowledge up to date without retraining the model and reduces hallucination by grounding answers in retrieved text."}}
{"timestamp": "2026-02-17T12:32:28.319208", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.8641, "answer_snippet": "Chunking documents is done to split long texts into pieces that fit the retrieval step and the context window. It helps maintain semantic units, reduces irrelevant text, and improves the precision of ..."}}
{"timestamp": "2026-02-17T12:32:30.058712", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 1.739, "answer_snippet": "Embeddings are dense vector representations of text, where each piece of text\u2014such as a sentence or paragraph\u2014is mapped to a fixed-size vector. Similar meanings produce similar vectors, allowing for s..."}}
{"timestamp": "2026-02-17T12:32:32.105766", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 2.0466, "answer_snippet": "Chunk overlap is used to reduce boundary effects when splitting documents into chunks. It helps maintain context by ensuring that important information at the edges of chunks is not lost, allowing for..."}}
{"timestamp": "2026-02-17T12:32:34.823082", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.7167, "answer_snippet": "Top-k in retrieval refers to the parameter k that determines how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, th..."}}
{"timestamp": "2026-02-17T12:32:35.713123", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": [], "hit": false, "time_sec": 0.8895, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:32:36.823483", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 1.1098, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:32:38.252268", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.4283, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries using the initial retrieved chunks or the model's intermediate answer and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T12:32:40.067788", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.8153, "answer_snippet": "Contrastive learning is the typical training objective for embedding models, where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T12:32:41.530355", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.4623, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T12:32:42.963580", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.4326, "answer_snippet": "The RAG practices guide recommends a default chunk size of 256 tokens and an overlap of 32 tokens."}}
{"timestamp": "2026-02-17T12:32:44.999550", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": ["llm_practices.txt", "llm_practices_11.txt"], "hit": true, "time_sec": 2.0354, "answer_snippet": "Query rewriting is the process of rephrasing the user question into a more retrieval-friendly form before embedding it. This technique can improve recall, allowing for better retrieval of relevant doc..."}}
{"timestamp": "2026-02-17T12:32:46.794432", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.7943, "answer_snippet": "You should use a flat index (exact search) for corpora under roughly 100k chunks and switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T12:32:48.989924", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 2.1949, "answer_snippet": "The incremental indexing rule defines that when you add or update documents, you should only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk ..."}}
{"timestamp": "2026-02-17T12:32:50.508812", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.5184, "answer_snippet": "You should store chunk_id in vector store metadata so the model can cite the specific source of the information, allowing users to check which chunks supported the answer."}}
{"timestamp": "2026-02-17T12:32:51.564165", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 1.0548, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:32:57.094590", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 5.5299, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:32:59.097564", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 2.0022, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:33:01.051128", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.8676861524581909, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.8747826814651489, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9547345638275146, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9929119944572449, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3597}}
{"timestamp": "2026-02-17T12:33:01.299137", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.8498893976211548, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8881575465202332, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9517967104911804, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.968418300151825, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2477}}
{"timestamp": "2026-02-17T12:33:01.520972", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.9735124111175537, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.134939432144165, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1717355251312256, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.172817349433899, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2213}}
{"timestamp": "2026-02-17T12:33:01.742708", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 1.099091649055481, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.102043867111206, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.1301696300506592, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.160104751586914, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2213}}
{"timestamp": "2026-02-17T12:33:01.962680", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.8572636842727661, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8801422119140625, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.9099944233894348, "chunk_id": "llm_practices_2", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.9214882850646973, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2196}}
{"timestamp": "2026-02-17T12:33:02.178282", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 1.0261214971542358, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.061389446258545, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0819519758224487, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0876665115356445, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2152}}
{"timestamp": "2026-02-17T12:33:02.391371", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8908637762069702, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.9426461458206177, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9729955196380615, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9861631393432617, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2127}}
{"timestamp": "2026-02-17T12:33:02.644930", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.9981955289840698, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.2010215520858765, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.228154182434082, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2492189407348633, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2531}}
{"timestamp": "2026-02-17T12:33:03.595638", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 1.0059945583343506, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0817798376083374, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.1226531267166138, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1438778638839722, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.9503}}
{"timestamp": "2026-02-17T12:33:03.836644", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 1.0037956237792969, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.3337920904159546, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.3458967208862305, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3545286655426025, "chunk_id": "machine_learning_0", "text_snippet": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can acce...", "source": "machine_learning.txt"}], "retrieved_sources": ["llm_concepts.txt", "machine_learning.txt"], "hit": true, "time_sec": 0.2405}}
{"timestamp": "2026-02-17T12:33:04.057495", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.8867570757865906, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.895190417766571, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.937854528427124, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.9402251243591309, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2203}}
{"timestamp": "2026-02-17T12:33:04.322200", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.3361999988555908, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3820767402648926, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.4117034673690796, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.4162814617156982, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2644}}
{"timestamp": "2026-02-17T12:33:04.539392", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.213672399520874, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3758494853973389, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2168}}
{"timestamp": "2026-02-17T12:33:04.846292", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 1.032606601715088, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3584580421447754, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.39247727394104, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.420637845993042, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3065}}
{"timestamp": "2026-02-17T12:33:05.070502", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 1.0612521171569824, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1369178295135498, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1883459091186523, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2369951009750366, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2237}}
{"timestamp": "2026-02-17T12:33:08.686668", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 2.1885, "answer_snippet": "RAG solves the problem of hallucination in language models by grounding answers in retrieved text, allowing the model to answer from a changing or large knowledge base without the need for retraining."}}
{"timestamp": "2026-02-17T12:33:11.430447", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.7434, "answer_snippet": "We chunk documents to split long texts into retrievable units, which helps in embedding and retrieving relevant information effectively. Chunking allows for better management of context and relevance ..."}}
{"timestamp": "2026-02-17T12:33:14.068155", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.6371, "answer_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector. Similar meanings produce similar vectors, allowing..."}}
{"timestamp": "2026-02-17T12:33:15.645092", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.5764, "answer_snippet": "Chunk overlap is used to reduce boundary effects when splitting documents into chunks. It helps avoid cutting sentences or topics in half, ensuring that semantic units remain intact and improving the ..."}}
{"timestamp": "2026-02-17T12:33:18.088778", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.4432, "answer_snippet": "Top-k in retrieval refers to the parameter that controls how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, the fi..."}}
{"timestamp": "2026-02-17T12:33:19.203598", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": [], "hit": false, "time_sec": 1.1142, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:33:20.236130", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 1.032, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:33:21.848355", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.6117, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries using the initial retrieved chunks or the model's intermediate answer and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T12:33:23.361742", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.5128, "answer_snippet": "Contrastive learning is the typical training objective for embedding models where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T12:33:24.742357", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.3801, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T12:33:25.922317", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.1794, "answer_snippet": "The RAG practices guide recommends experimenting with chunk sizes of 200\u2013500 tokens and an overlap of 20\u201350 tokens."}}
{"timestamp": "2026-02-17T12:33:27.405372", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.4825, "answer_snippet": "Query rewriting is the process of rephrasing the user question into a more retrieval-friendly form before embedding it. This technique can improve recall, allowing for better retrieval of relevant chu..."}}
{"timestamp": "2026-02-17T12:33:29.349096", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.9432, "answer_snippet": "Use a flat index for latency-sensitive applications with corpora under roughly 100k chunks. Switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T12:33:30.905193", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.5556, "answer_snippet": "The incremental indexing rule states that when you add or update documents, you should only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk I..."}}
{"timestamp": "2026-02-17T12:33:32.408110", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.5023, "answer_snippet": "Storing chunk_id in vector store metadata is important so that the model can reference the correct document and the user can verify the information provided. This allows for accurate citations and ens..."}}
{"timestamp": "2026-02-17T12:33:33.473223", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 1.0646, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:33:34.492120", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 1.0183, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:33:35.492477", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 0.9999, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:33:37.462709", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.9097669124603271, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9298489093780518, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9743603467941284, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.045147180557251, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.5445}}
{"timestamp": "2026-02-17T12:33:37.679204", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.9812992215156555, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.9834392666816711, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0596494674682617, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1843620538711548, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2162}}
{"timestamp": "2026-02-17T12:33:37.926263", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 1.1718655824661255, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.2701833248138428, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.3304991722106934, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.330528736114502, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2466}}
{"timestamp": "2026-02-17T12:33:38.151897", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 1.1365957260131836, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.1446807384490967, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1533852815628052, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2140055894851685, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2251}}
{"timestamp": "2026-02-17T12:33:38.368570", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.9588179588317871, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0168321132659912, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.074376106262207, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.080352544784546, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2161}}
{"timestamp": "2026-02-17T12:33:38.598737", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 1.0613429546356201, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0984630584716797, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.10073983669281, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1852625608444214, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2297}}
{"timestamp": "2026-02-17T12:33:38.819366", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.9403129816055298, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.9816773533821106, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.9996827840805054, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.062436580657959, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2202}}
{"timestamp": "2026-02-17T12:33:39.040523", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.9982241988182068, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.2635771036148071, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.263685703277588, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2886626720428467, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2206}}
{"timestamp": "2026-02-17T12:33:39.271719", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 1.0058780908584595, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1461427211761475, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2816566228866577, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.31123948097229, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2307}}
{"timestamp": "2026-02-17T12:33:39.695024", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 1.1554183959960938, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.3343884944915771, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.4265494346618652, "chunk_id": "machine_learning_0", "text_snippet": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can acce...", "source": "machine_learning.txt"}, {"rank": 4, "score": 1.4670770168304443, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}], "retrieved_sources": ["machine_learning.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.4228}}
{"timestamp": "2026-02-17T12:33:39.945184", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.7659802436828613, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.767387866973877, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9696542024612427, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.973135232925415, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2497}}
{"timestamp": "2026-02-17T12:33:40.198852", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.3822940587997437, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.4609521627426147, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.4630379676818848, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.4698841571807861, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": false, "time_sec": 0.2533}}
{"timestamp": "2026-02-17T12:33:40.417993", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.4576796293258667, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2187}}
{"timestamp": "2026-02-17T12:33:40.625415", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 1.2120544910430908, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3592984676361084, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.4204500913619995, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.4451996088027954, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.207}}
{"timestamp": "2026-02-17T12:33:40.841158", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 1.1906582117080688, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.1928040981292725, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.2388255596160889, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3605507612228394, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2155}}
{"timestamp": "2026-02-17T12:33:46.048948", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 3.5487, "answer_snippet": "RAG solves the problem of hallucination in language models by grounding answers in retrieved text, allowing the model to cite sources and stay up to date without needing to retrain on a changing or la..."}}
{"timestamp": "2026-02-17T12:33:48.218515", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 2.1691, "answer_snippet": "We chunk documents to split long texts into retrievable units that fit the retrieval step and the context window. This helps maintain semantic integrity, improves retrieval precision, and ensures that..."}}
{"timestamp": "2026-02-17T12:33:50.444599", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.2255, "answer_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector. Similar meanings produce similar vectors, allowing..."}}
{"timestamp": "2026-02-17T12:33:51.969965", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.5248, "answer_snippet": "Chunk overlap is used to reduce the risk of losing context when splitting documents into chunks. It helps ensure that sentences or concepts are not cut off in the middle, thereby improving the precisi..."}}
{"timestamp": "2026-02-17T12:33:55.183353", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 3.2128, "answer_snippet": "Top-k in retrieval refers to the parameter that controls how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, the fi..."}}
{"timestamp": "2026-02-17T12:33:56.086057", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": [], "hit": false, "time_sec": 0.9021, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:33:57.110893", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 1.0242, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:33:58.731452", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.62, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries using the initial retrieved chunks or the model's intermediate answer and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T12:34:00.536101", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.8042, "answer_snippet": "Contrastive learning is the typical training objective for embedding models where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T12:34:03.244537", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 2.7079, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T12:34:04.521768", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.2767, "answer_snippet": "The RAG practices guide recommends a default chunk size of 256 tokens and an overlap of 32 tokens."}}
{"timestamp": "2026-02-17T12:34:05.786135", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": [], "hit": false, "time_sec": 1.2638, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:34:07.678806", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.8923, "answer_snippet": "Use a flat index (exact search) for corpora under roughly 100k chunks and switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T12:34:09.269715", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.5903, "answer_snippet": "The incremental indexing rule states that when you add or update documents, you should only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk I..."}}
{"timestamp": "2026-02-17T12:34:11.027897", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.7576, "answer_snippet": "You should store chunk_id in vector store metadata so that the model can cite the correct document and the user can verify the sources that supported the answer."}}
{"timestamp": "2026-02-17T12:34:11.948555", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 0.9201, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:34:12.946374", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 0.9973, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:34:13.822235", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 0.8753, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:37:17.731663", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.7983037233352661, "chunk_id": "llm_practices_9", "text_snippet": "In practice, RAG and fine-tuning address different needs. RAG is best when you have a changing or large knowledge base and want the model to answer from it without retraining. Fine-tuning is best when...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.8611981868743896, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.862047016620636, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8998799920082092, "chunk_id": "llm_concepts_3", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3824}}
{"timestamp": "2026-02-17T12:37:17.952137", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.6431180834770203, "chunk_id": "llm_practices_2", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.6927129626274109, "chunk_id": "llm_concepts_6", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.8942553997039795, "chunk_id": "llm_concepts_10", "text_snippet": "This paragraph is intentionally off-topic. When building a RAG pipeline, you might ingest documents that contain boilerplate, copyright notices, or unrelated sections. Good preprocessing and chunking ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9364926815032959, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2202}}
{"timestamp": "2026-02-17T12:37:18.204654", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.7766802906990051, "chunk_id": "llm_concepts_5", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8831441402435303, "chunk_id": "llm_practices_1", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1718655824661255, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2315666675567627, "chunk_id": "llm_concepts_11", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2521}}
{"timestamp": "2026-02-17T12:37:18.524815", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 0.8906810879707336, "chunk_id": "llm_practices_2", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.028498649597168, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0454375743865967, "chunk_id": "llm_concepts_6", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1408716440200806, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3198}}
{"timestamp": "2026-02-17T12:37:18.775528", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.60416579246521, "chunk_id": "llm_concepts_7", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.7793642282485962, "chunk_id": "llm_practices_5", "text_snippet": "The top-k parameter controls how many retrieved chunks are passed into the prompt. With k=1 you only send the single best-matching chunk; with k=5 you send the five best. Higher k improves the chance ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8728569746017456, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9325721263885498, "chunk_id": "llm_practices_7", "text_snippet": "\u2022 RAG reduces hallucination by grounding answers in retrieved chunks.\n\u2022 Ask the model to cite chunks; consider verification and human review.\n\u2022 Cleaning input documents can improve retrieval and thus ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2504}}
{"timestamp": "2026-02-17T12:37:19.019616", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 1.0001227855682373, "chunk_id": "llm_practices_9", "text_snippet": "In practice, RAG and fine-tuning address different needs. RAG is best when you have a changing or large knowledge base and want the model to answer from it without retraining. Fine-tuning is best when...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.000195026397705, "chunk_id": "llm_practices_8", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0370783805847168, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.044384479522705, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2437}}
{"timestamp": "2026-02-17T12:37:19.305363", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.7990533113479614, "chunk_id": "llm_practices_9", "text_snippet": "In practice, RAG and fine-tuning address different needs. RAG is best when you have a changing or large knowledge base and want the model to answer from it without retraining. Fine-tuning is best when...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.8847654461860657, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9251013994216919, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9280808568000793, "chunk_id": "llm_practices_8", "text_snippet": "Evaluating RAG systems requires metrics at two levels: retrieval and generation. Retrieval recall measures whether the passage that contains the answer appears in the top-k. Retrieval precision measur...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2853}}
{"timestamp": "2026-02-17T12:37:19.526302", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.9982999563217163, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0855352878570557, "chunk_id": "llm_concepts_8", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.164849877357483, "chunk_id": "llm_concepts_3", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1787117719650269, "chunk_id": "llm_concepts_7", "text_snippet": "The value k in \"top-k retrieval\" is how many chunks or passages you fetch per query. When k equals 1 you only get the single best-matching chunk; when k is 5 or 10 you get the five or ten best. Larger...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.2205}}
{"timestamp": "2026-02-17T12:37:19.773883", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 0.9603841304779053, "chunk_id": "llm_concepts_5", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0058780908584595, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0338053703308105, "chunk_id": "llm_practices_1", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.087310552597046, "chunk_id": "llm_concepts_2", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2471}}
{"timestamp": "2026-02-17T12:37:20.061920", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 0.8788719773292542, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.2984271049499512, "chunk_id": "llm_concepts_11", "text_snippet": "In summary, the main ideas are as follows. LLMs rely on transformers, tokenization, context windows, prompt engineering, and fine-tuning. RAG means retrieving relevant chunks, adding them to the promp...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.3201960325241089, "chunk_id": "llm_concepts_2", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3342782258987427, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 0.2876}}
{"timestamp": "2026-02-17T12:37:20.286490", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.6414488554000854, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.8000329732894897, "chunk_id": "llm_practices_2", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.8003292083740234, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.8363180160522461, "chunk_id": "llm_practices_9", "text_snippet": "In practice, RAG and fine-tuning address different needs. RAG is best when you have a changing or large knowledge base and want the model to answer from it without retraining. Fine-tuning is best when...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2241}}
{"timestamp": "2026-02-17T12:37:20.503586", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.2459241151809692, "chunk_id": "llm_practices_7", "text_snippet": "\u2022 RAG reduces hallucination by grounding answers in retrieved chunks.\n\u2022 Ask the model to cite chunks; consider verification and human review.\n\u2022 Cleaning input documents can improve retrieval and thus ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2842625379562378, "chunk_id": "llm_practices_11", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.3823955059051514, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3844127655029297, "chunk_id": "llm_concepts_3", "text_snippet": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with generation. In formal terms, RAG can be described as modelling p(answer | query, retrieve(query)), where retrieve(que...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2167}}
{"timestamp": "2026-02-17T12:37:20.834484", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.0001599788665771, "chunk_id": "llm_practices_11", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.2951699495315552, "chunk_id": "llm_concepts_5", "text_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector, for example 1536 dimensions for OpenAI's text-embe...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.3731310367584229, "chunk_id": "llm_practices_1", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.4645582437515259, "chunk_id": "llm_practices_7", "text_snippet": "\u2022 RAG reduces hallucination by grounding answers in retrieved chunks.\n\u2022 Ask the model to cite chunks; consider verification and human review.\n\u2022 Cleaning input documents can improve retrieval and thus ...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.3305}}
{"timestamp": "2026-02-17T12:37:21.048162", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 0.8223041296005249, "chunk_id": "llm_practices_11", "text_snippet": "Exclusive to this file: the RAG Practices Guide defines the \"incremental indexing rule\": when you add or update documents, only embed and index the new or changed chunks rather than re-processing the ...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3592984676361084, "chunk_id": "llm_concepts_12", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.361822485923767, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.3815557956695557, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 0.2132}}
{"timestamp": "2026-02-17T12:37:21.313631", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 1.1320381164550781, "chunk_id": "llm_practices_1", "text_snippet": "Embeddings turn text into fixed-length vectors. Similar texts get similar vectors, so you can find relevant passages by comparing the query embedding to stored chunk embeddings (e.g. with cosine simil...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.1538238525390625, "chunk_id": "llm_practices_4", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.187307357788086, "chunk_id": "llm_practices_10", "text_snippet": "Summary of overlapping points with other RAG and LLM guides: RAG combines retrieval (often over embeddings in a vector store) with generation. Chunking and top-k are key parameters. Use the same embed...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.20559561252594, "chunk_id": "llm_practices_2", "text_snippet": "Chunking is how you split documents into pieces that will be embedded and retrieved. If chunks are too small you lose surrounding context; if they are too large each chunk contains more irrelevant tex...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.2651}}
{"timestamp": "2026-02-17T12:37:25.707363", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 3.0778, "answer_snippet": "RAG (Retrieval-Augmented Generation) solves the problem of keeping knowledge up to date without retraining the model and reduces hallucination by grounding answers in retrieved text."}}
{"timestamp": "2026-02-17T12:37:27.360464", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.6528, "answer_snippet": "Chunking documents is done to split long texts into pieces that fit the retrieval step and the context window. It helps maintain semantic units, reduces the chance of losing surrounding context, and i..."}}
{"timestamp": "2026-02-17T12:37:29.089961", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 1.7289, "answer_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector. Similar meanings produce similar vectors, allowing..."}}
{"timestamp": "2026-02-17T12:37:30.709323", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.6188, "answer_snippet": "Chunk overlap is used to reduce boundary effects when splitting documents into chunks. It helps maintain context by ensuring that sentences or topics are not cut in half, allowing for more coherent re..."}}
{"timestamp": "2026-02-17T12:37:33.434953", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.7252, "answer_snippet": "Top-k in retrieval refers to the parameter k that determines how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, th..."}}
{"timestamp": "2026-02-17T12:37:35.199310", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": [], "hit": false, "time_sec": 1.7638, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:37:36.648418", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 1.4486, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:37:38.456604", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.8076, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries using the initial retrieved chunks or the model's intermediate answer and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T12:37:39.729155", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.272, "answer_snippet": "Contrastive learning is the typical training objective for embedding models where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T12:37:41.143210", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.4135, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T12:37:42.447605", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.3038, "answer_snippet": "The RAG practices guide recommends a default chunk size of 256 tokens and an overlap of 32 tokens."}}
{"timestamp": "2026-02-17T12:37:44.291753", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": ["llm_practices.txt", "llm_practices_11.txt"], "hit": true, "time_sec": 1.8436, "answer_snippet": "Query rewriting is the process of rephrasing the user question into a more retrieval-friendly form before embedding it. This technique can improve recall, allowing the retrieval system to better fetch..."}}
{"timestamp": "2026-02-17T12:37:45.574840", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.2825, "answer_snippet": "You should use a flat index (exact search) for corpora under roughly 100k chunks and switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T12:37:47.401921", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.8265, "answer_snippet": "The incremental indexing rule defines that when you add or update documents, you should only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk ..."}}
{"timestamp": "2026-02-17T12:37:48.941587", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.5391, "answer_snippet": "You should store chunk_id in vector store metadata so the model can cite the specific source of the information, such as \"according to chunk 2 of llm_practices_dirty.txt.\""}}
{"timestamp": "2026-02-17T12:37:50.607998", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 1.6659, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:37:51.599232", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 0.9907, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:37:52.943699", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 1.3441, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:37:55.047258", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.8676861524581909, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.8747826814651489, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9547345638275146, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9929119944572449, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3929}}
{"timestamp": "2026-02-17T12:37:55.538774", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.8498893976211548, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8881575465202332, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9517967104911804, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.968418300151825, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.4912}}
{"timestamp": "2026-02-17T12:37:55.757868", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 0.9735124111175537, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.134939432144165, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1717355251312256, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.172817349433899, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2187}}
{"timestamp": "2026-02-17T12:37:56.425016", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 1.099091649055481, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.102043867111206, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.1301696300506592, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.160104751586914, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.6667}}
{"timestamp": "2026-02-17T12:37:56.652960", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.8572636842727661, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.8801422119140625, "chunk_id": "llm_concepts_3", "text_snippet": "Chunking splits long documents into pieces that fit the retrieval step and the context window. There is no single best chunk size; it depends on your documents and the kinds of queries you expect.\n\n\u2022 ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.9099944233894348, "chunk_id": "llm_practices_2", "text_snippet": "\u2022 Pipeline: ingest \u2192 clean \u2192 chunk \u2192 embed \u2192 store; at query: embed query \u2192 retrieve top-k \u2192 prompt with chunks \u2192 generate.\n\u2022 Chunk size and top-k are the main levers; tune them on a small set of ques...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.9214882850646973, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2275}}
{"timestamp": "2026-02-17T12:37:57.029686", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 1.0261214971542358, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.061389446258545, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.0819519758224487, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.0876665115356445, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3763}}
{"timestamp": "2026-02-17T12:37:57.242759", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.8908637762069702, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.9426461458206177, "chunk_id": "llm_practices_3", "text_snippet": "Hallucination\u2014when the model generates plausible but false or unsupported claims\u2014is a major risk when using LLMs. RAG mitigates this by constraining the model to answer from retrieved text. Other miti...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9729955196380615, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.9861631393432617, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2126}}
{"timestamp": "2026-02-17T12:37:57.479143", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.9981955289840698, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.2010215520858765, "chunk_id": "llm_concepts_4", "text_snippet": "\u2022 Short factual questions: k = 2 or 3 is often enough.\n\u2022 Complex or multi-part questions: k = 5\u201310 or multi-hop retrieval may help.\n\u2022 Always tune k on a validation set and measure accuracy or faithful...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.228154182434082, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2492189407348633, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2359}}
{"timestamp": "2026-02-17T12:37:57.745085", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 1.0059945583343506, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0817798376083374, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.1226531267166138, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.1438778638839722, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2655}}
{"timestamp": "2026-02-17T12:37:57.961783", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 1.0037956237792969, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.3337920904159546, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.3458967208862305, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3545286655426025, "chunk_id": "machine_learning_0", "text_snippet": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can acce...", "source": "machine_learning.txt"}], "retrieved_sources": ["llm_concepts.txt", "machine_learning.txt"], "hit": true, "time_sec": 0.2162}}
{"timestamp": "2026-02-17T12:37:58.257025", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.8867570757865906, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.895190417766571, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.937854528427124, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 0.9402251243591309, "chunk_id": "llm_concepts_5", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: does the answer stay grounded in the retrieved text?\n(4) Answer r...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2949}}
{"timestamp": "2026-02-17T12:37:58.583811", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.3362326622009277, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3820585012435913, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.4116894006729126, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.4162732362747192, "chunk_id": "llm_concepts_1", "text_snippet": "Context window refers to the maximum number of tokens the model can take as input at once. Early models had windows of a few thousand tokens; newer ones support tens or hundreds of thousands. Long con...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3263}}
{"timestamp": "2026-02-17T12:37:58.897081", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.2137507200241089, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3759181499481201, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.3128}}
{"timestamp": "2026-02-17T12:37:59.133646", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 1.032606601715088, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.3584580421447754, "chunk_id": "llm_concepts_6", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.39247727394104, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.420637845993042, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.236}}
{"timestamp": "2026-02-17T12:37:59.354764", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 1.0612521171569824, "chunk_id": "llm_concepts_2", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1369178295135498, "chunk_id": "llm_practices_1", "text_snippet": "\u2022 Use one embedding model for both indexing and querying.\n\u2022 Vector stores: ChromaDB, FAISS, Pinecone, Weaviate, Qdrant; choose by scale and features.\n\u2022 Top-k: number of chunks returned per query; larg...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1883459091186523, "chunk_id": "llm_practices_5", "text_snippet": "\u2022 RAG: retrieve relevant chunks, then generate; reduces hallucination; needs chunking, embeddings, vector store, top-k.\n\u2022 Chunk size and overlap: experiment with 200\u2013500 tokens and overlap 20\u201350.\n\u2022 To...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.2369951009750366, "chunk_id": "llm_practices_4", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2206}}
{"timestamp": "2026-02-17T12:38:02.781639", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.7945, "answer_snippet": "RAG solves the problem of hallucination in language models by grounding answers in retrieved text, allowing the model to answer from a changing or large knowledge base without the need for retraining."}}
{"timestamp": "2026-02-17T12:38:04.921941", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.1398, "answer_snippet": "We chunk documents to split long texts into retrievable units, which helps in embedding and retrieving relevant information effectively. Chunking allows for better management of context and relevance ..."}}
{"timestamp": "2026-02-17T12:38:07.294570", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.372, "answer_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector. Similar meanings produce similar vectors, allowing..."}}
{"timestamp": "2026-02-17T12:38:08.861975", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.5668, "answer_snippet": "Chunk overlap is used to reduce boundary effects when splitting documents into chunks. It helps avoid cutting sentences or topics in half, ensuring that semantic units remain intact and improving the ..."}}
{"timestamp": "2026-02-17T12:38:11.608299", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.7457, "answer_snippet": "Top-k in retrieval refers to the parameter that controls how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, the fi..."}}
{"timestamp": "2026-02-17T12:38:12.536218", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": [], "hit": false, "time_sec": 0.9274, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:38:13.629843", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 1.0931, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:38:15.253645", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.6232, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries using the initial retrieved chunks or the model's intermediate answer and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T12:38:16.687221", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.4331, "answer_snippet": "Contrastive learning is the typical training objective for embedding models where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T12:38:18.430133", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.7424, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T12:38:19.592910", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.1622, "answer_snippet": "The RAG practices guide recommends experimenting with chunk sizes of 200\u2013500 tokens and an overlap of 20\u201350 tokens."}}
{"timestamp": "2026-02-17T12:38:21.125963", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.5325, "answer_snippet": "Query rewriting is the process of rephrasing the user question into a more retrieval-friendly form before embedding it. This technique can improve recall, allowing for better retrieval of relevant chu..."}}
{"timestamp": "2026-02-17T12:38:22.414465", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.288, "answer_snippet": "Use a flat index (exact search) for latency-sensitive applications with corpora under roughly 100k chunks. Switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T12:38:23.894411", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.4794, "answer_snippet": "The incremental indexing rule states that when you add or update documents, you should only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk I..."}}
{"timestamp": "2026-02-17T12:38:25.493794", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.5989, "answer_snippet": "Storing chunk_id in vector store metadata is important so that the model can reference the correct document and the user can verify the information provided. This allows for accurate citations and ens..."}}
{"timestamp": "2026-02-17T12:38:26.415373", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 0.921, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:38:27.275377", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 0.8595, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:38:28.164784", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 0.8889, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:38:30.113319", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "results": [{"rank": 1, "score": 0.9097669124603271, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 0.9298489093780518, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9743603467941284, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.045147180557251, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.4619}}
{"timestamp": "2026-02-17T12:38:30.336014", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "results": [{"rank": 1, "score": 0.9812992215156555, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.9834392666816711, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.0596494674682617, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1843620538711548, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2224}}
{"timestamp": "2026-02-17T12:38:30.560139", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "results": [{"rank": 1, "score": 1.1718655824661255, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.2701833248138428, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.3304991722106934, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.330528736114502, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2237}}
{"timestamp": "2026-02-17T12:38:30.788326", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "results": [{"rank": 1, "score": 1.1365957260131836, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.1446807384490967, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.1533852815628052, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.2140055894851685, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2278}}
{"timestamp": "2026-02-17T12:38:31.035956", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "results": [{"rank": 1, "score": 0.9588179588317871, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0168321132659912, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.074376106262207, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.080352544784546, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2472}}
{"timestamp": "2026-02-17T12:38:31.278108", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "results": [{"rank": 1, "score": 1.0613429546356201, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.0984630584716797, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.10073983669281, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.1852625608444214, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2417}}
{"timestamp": "2026-02-17T12:38:31.538758", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "results": [{"rank": 1, "score": 0.9403129816055298, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.9816773533821106, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 0.9996827840805054, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.062436580657959, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2602}}
{"timestamp": "2026-02-17T12:38:31.763727", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "results": [{"rank": 1, "score": 0.9982999563217163, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.2636770009994507, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.263858437538147, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.288790225982666, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2246}}
{"timestamp": "2026-02-17T12:38:31.991899", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "results": [{"rank": 1, "score": 1.0058780908584595, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.1461427211761475, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.2816566228866577, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.31123948097229, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2277}}
{"timestamp": "2026-02-17T12:38:32.281310", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "results": [{"rank": 1, "score": 1.1553151607513428, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.3342782258987427, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.426542043685913, "chunk_id": "machine_learning_0", "text_snippet": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can acce...", "source": "machine_learning.txt"}, {"rank": 4, "score": 1.4669859409332275, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}], "retrieved_sources": ["machine_learning.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.289}}
{"timestamp": "2026-02-17T12:38:32.502171", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "results": [{"rank": 1, "score": 0.7659802436828613, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 2, "score": 0.767387866973877, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 3, "score": 0.9696542024612427, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 0.973135232925415, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2204}}
{"timestamp": "2026-02-17T12:38:32.723344", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "results": [{"rank": 1, "score": 1.3823955059051514, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 2, "score": 1.4610199928283691, "chunk_id": "llm_concepts_2", "text_snippet": "RAG stands for Retrieval-Augmented Generation: you retrieve first, then generate. That way the model can cite sources and stay up to date. Earlier in this document we said that chunking long documents...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.4630115032196045, "chunk_id": "llm_concepts_0", "text_snippet": "Large Language Models (LLMs) are neural networks trained on vast amounts of text to understand and generate human language. They use the transformer architecture, which relies on self-attention to pro...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.4699788093566895, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_concepts.txt"], "hit": false, "time_sec": 0.2207}}
{"timestamp": "2026-02-17T12:38:32.966792", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "results": [{"rank": 1, "score": 1.4576796293258667, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 0.243}}
{"timestamp": "2026-02-17T12:38:33.498409", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "results": [{"rank": 1, "score": 1.211669921875, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.359065294265747, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}, {"rank": 3, "score": 1.4199297428131104, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 4, "score": 1.444165825843811, "chunk_id": "llm_practices_1", "text_snippet": "This document only (RAG Practices Guide): for most RAG tutorials we recommend starting with chunk_size=256 tokens, overlap=32, and k=3 as default hyperparameters. If answers are incomplete, increase k...", "source": "llm_practices.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.5312}}
{"timestamp": "2026-02-17T12:38:33.729415", "event": "retrieval_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "results": [{"rank": 1, "score": 1.1906582117080688, "chunk_id": "llm_practices_2", "text_snippet": "(1) Retrieval recall: is the gold passage in the top-k?\n(2) Retrieval precision: how many of the top-k are relevant?\n(3) Faithfulness: is the answer grounded in the retrieved text?\n(4) Answer relevanc...", "source": "llm_practices.txt"}, {"rank": 2, "score": 1.1928040981292725, "chunk_id": "llm_practices_0", "text_snippet": "This document describes practical aspects of building systems with large language models and retrieval-augmented generation. The goal is to give you overlapping coverage of the same topics you might f...", "source": "llm_practices.txt"}, {"rank": 3, "score": 1.2388255596160889, "chunk_id": "llm_concepts_1", "text_snippet": "\u2022 Document chunking: how you split long texts into retrievable units.\n\u2022 Embedding model: turns text into vectors; use the same model at index and query time.\n\u2022 Vector store: ChromaDB, FAISS, Pinecone,...", "source": "llm_concepts.txt"}, {"rank": 4, "score": 1.3605507612228394, "chunk_id": "llm_concepts_3", "text_snippet": "\u2022 LLMs: transformers, tokenization, context windows, prompt engineering, fine-tuning.\n\u2022 RAG: retrieve chunks, add to prompt, generate; reduces hallucination; needs chunking, embeddings, vector store, ...", "source": "llm_concepts.txt"}], "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 0.2305}}
{"timestamp": "2026-02-17T12:38:37.003918", "event": "generation_eval", "data": {"backend": "chroma", "question": "What problem does RAG solve?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 1.5985, "answer_snippet": "RAG solves the problem of hallucination in language models by grounding answers in retrieved text, allowing the model to cite sources and stay up to date without needing retraining."}}
{"timestamp": "2026-02-17T12:38:39.561029", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why do we chunk documents?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 2.5567, "answer_snippet": "We chunk documents to split long texts into retrievable units that fit the retrieval step and the context window. This helps maintain semantic coherence, avoids losing context, and improves the precis..."}}
{"timestamp": "2026-02-17T12:38:41.515687", "event": "generation_eval", "data": {"backend": "chroma", "question": "What are embeddings?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 1.9543, "answer_snippet": "Embeddings are dense vector representations of text. Each piece of text\u2014whether a sentence, a paragraph, or a chunk\u2014is mapped to a fixed-size vector. Similar meanings produce similar vectors, allowing..."}}
{"timestamp": "2026-02-17T12:38:43.054463", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is chunk overlap used for?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.5382, "answer_snippet": "Chunk overlap is used to reduce the risk of losing context when splitting documents into chunks. It helps ensure that sentences or concepts are not cut off in the middle, thereby improving the precisi..."}}
{"timestamp": "2026-02-17T12:38:45.582305", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is top-k in retrieval?", "retrieved_sources": ["llm_concepts.txt", "llm_practices.txt"], "hit": true, "time_sec": 2.5273, "answer_snippet": "Top-k in retrieval refers to the parameter that controls how many chunks or passages are fetched per query. When k equals 1, only the single best-matching chunk is retrieved; when k is 5 or 10, the fi..."}}
{"timestamp": "2026-02-17T12:38:46.588298", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the formal definition of RAG in terms of probability?", "retrieved_sources": [], "hit": false, "time_sec": 1.0054, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:38:47.636917", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the information bottleneck in RAG?", "retrieved_sources": [], "hit": false, "time_sec": 1.0481, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:38:49.297616", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is multi-hop retrieval defined?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.6601, "answer_snippet": "Multi-hop retrieval is defined as issuing follow-up queries using the initial retrieved chunks or the model's intermediate answer and then combining results from multiple retrieval rounds."}}
{"timestamp": "2026-02-17T12:38:50.959634", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is contrastive learning in the context of embedding models?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.6615, "answer_snippet": "Contrastive learning is the typical training objective for embedding models where relevant (query, passage) pairs are pulled together in vector space, and irrelevant pairs are pushed apart."}}
{"timestamp": "2026-02-17T12:38:52.240388", "event": "generation_eval", "data": {"backend": "chroma", "question": "How is self-attention formally computed in transformers?", "retrieved_sources": ["llm_concepts.txt"], "hit": true, "time_sec": 1.2802, "answer_snippet": "Self-attention computes a weighted sum of value vectors where the weights are derived from query-key dot products, then scaled by the square root of the head dimension."}}
{"timestamp": "2026-02-17T12:38:53.399443", "event": "generation_eval", "data": {"backend": "chroma", "question": "What default chunk size and overlap does the RAG practices guide recommend?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.1585, "answer_snippet": "The RAG practices guide recommends a default chunk size of 256 tokens and an overlap of 32 tokens."}}
{"timestamp": "2026-02-17T12:38:54.358059", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is query rewriting and why use it?", "retrieved_sources": [], "hit": false, "time_sec": 0.9583, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:38:56.039422", "event": "generation_eval", "data": {"backend": "chroma", "question": "When should you use a flat index versus HNSW or IVF?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.6808, "answer_snippet": "Use a flat index (exact search) for corpora under roughly 100k chunks and switch to HNSW or IVF when the index grows larger."}}
{"timestamp": "2026-02-17T12:38:57.750151", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the incremental indexing rule?", "retrieved_sources": ["llm_practices.txt"], "hit": true, "time_sec": 1.7102, "answer_snippet": "The incremental indexing rule states that when you add or update documents, only embed and index the new or changed chunks rather than re-processing the full corpus, as long as your chunk IDs are stab..."}}
{"timestamp": "2026-02-17T12:38:59.245527", "event": "generation_eval", "data": {"backend": "chroma", "question": "Why should you store chunk_id in vector store metadata?", "retrieved_sources": ["llm_practices.txt", "llm_concepts.txt"], "hit": true, "time_sec": 1.4948, "answer_snippet": "You should store chunk_id in vector store metadata so that the model can cite the correct document and the user can verify the sources that supported the answer."}}
{"timestamp": "2026-02-17T12:39:00.208538", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the capital of France?", "retrieved_sources": [], "hit": true, "time_sec": 0.9624, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:39:01.068611", "event": "generation_eval", "data": {"backend": "chroma", "question": "How do you bake a chocolate cake?", "retrieved_sources": [], "hit": true, "time_sec": 0.8595, "answer_snippet": "I do not have enough information."}}
{"timestamp": "2026-02-17T12:39:01.949261", "event": "generation_eval", "data": {"backend": "chroma", "question": "What is the population of Tokyo?", "retrieved_sources": [], "hit": true, "time_sec": 0.8801, "answer_snippet": "I do not have enough information."}}
